{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "$\n",
        "question: [N, n, Tq, H]\\\\\n",
        "answer: [N, n, Ta, H]\n",
        "$\n",
        "\n",
        "$(matmul): H -> H'$\n",
        "\n",
        "$\n",
        "q: [N, n, Tq, H'] \\\\\n",
        "k: [N, n, Ta, H'] \\\\\n",
        "v: [N, n, Ta, H']\n",
        "$\n",
        "\n",
        "\n",
        "$\n",
        "scores = qk = [N, n, Tq, Ta] \\\\\n",
        "q\\_query = [N, n, Tq, Ta], ~ sum(axis=Ta) == 1 \\\\\n",
        "k\\_query = [N, n, Tq, Ta], ~ sum(axis=Tq) == 1 \\\\\n",
        "-> E1 = sum(q\\_query@k * k\\_query): [N, n, Tq, H'] -> [N, n, 1, H'] \\\\\n",
        "-> E2 = sum(q^T@k\\_query * q\\_query): [N, n, H', Ta] -> [N, n, H', 1]\n",
        "$\n",
        "\n",
        "$\n",
        "-> O = [N, n, 3, H] \\\\\n",
        "-> FC -> [N, n, H]\n",
        "$\n",
        "\n",
        "-> output = $[[CLS], attn1, attn2]$"
      ],
      "metadata": {
        "id": "qRlPHGPCJIPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUzZ0zo25R-O",
        "outputId": "9d17f36b-9d90-4596-e9a4-91da2cdfc15f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: sentencepiece, safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as tf\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "qNNBgA8CJHl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
        "from transformers.tokenization_utils_base import BatchEncoding"
      ],
      "metadata": {
        "id": "TDvIoYX1sCQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "zu850iN_Hra0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = AutoModel.from_pretrained(\"xlm-roberta-large\", output_hidden_states=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
      ],
      "metadata": {
        "id": "2Sq9Y0TzsDiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardLayer(nn.Module):\n",
        "\tdef __init__(self, in_units, hidden_units, out_units, dropout_rate=0.1):\n",
        "\t\tsuper(FeedForwardLayer, self).__init__()\n",
        "\n",
        "\t\tself.fc = nn.Linear(in_units, hidden_units)\n",
        "\t\tself.drop = nn.Dropout(dropout_rate)\n",
        "\t\tself.out_fc = nn.Linear(hidden_units, out_units)\n",
        "\n",
        "\tdef forward(self, inputs):\n",
        "\t\tx = tf.gelu(self.fc(inputs))\n",
        "\t\tx = self.drop(x)\n",
        "\t\tx = self.out_fc(x)\n",
        "\t\treturn x"
      ],
      "metadata": {
        "id": "9njtC4fcaHeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MergeMultiHeadAttention(nn.Module):\n",
        "\tdef __init__(self, feature_units, attention_units, num_heads):\n",
        "\t\tsuper(MergeMultiHeadAttention, self).__init__()\n",
        "\t\tself.num_heads = num_heads\n",
        "\t\tself.attention_units = attention_units\n",
        "\n",
        "\t\tassert attention_units % self.num_heads == 0\n",
        "\n",
        "\t\tself.depth = attention_units // self.num_heads\n",
        "\n",
        "\t\tself.Wq = nn.Linear(feature_units, attention_units)\n",
        "\t\tself.Wk = nn.Linear(feature_units, attention_units)\n",
        "\n",
        "\tdef split_heads(self, x):\n",
        "\t\t# 4D Batch: [N, n, Tq, H'] -> 5D:  -> [N, n, H, Tq, D]\n",
        "\t\tN, n, T, _ = x.shape\n",
        "\n",
        "\t\tx = x.view(N, n, T, self.num_heads, -1)\n",
        "\t\treturn torch.transpose(x, 3, 2)\n",
        "\n",
        "\tdef merge_heads(self, x):\n",
        "\t\tN, n, h, _ = x.shape\n",
        "\t\tx = x.view(N, n, -1)\n",
        "\t\treturn x\n",
        "\n",
        "\tdef forward(self, q, k, q_mask=None, k_mask=None):\n",
        "\t\t# mask_q: [N, Tq]\n",
        "\t\t# mask_k: [N, Tk]\n",
        "\t\twq = self.Wq(q)\n",
        "\t\twk = self.Wk(k)\n",
        "\n",
        "\t\twq = self.split_heads(wq)\n",
        "\t\twk = self.split_heads(wk)\n",
        "\t\tq_transposed = torch.transpose(wq, -1, -2)\n",
        "\t\tk_transposed = torch.transpose(wk, -1, -2)\n",
        "\n",
        "\t\tmatmul_qk = torch.matmul(wq, k_transposed)  # [N, n, H, Tq, Tv]\n",
        "\n",
        "\t\tscores = matmul_qk / torch.sqrt(torch.tensor(self.attention_units, dtype=torch.float32))\n",
        "\t\tscores_q_masked = scores\n",
        "\t\tscores_k_masked = scores\n",
        "\n",
        "\t\tif q_mask is not None:\n",
        "\t\t\tq_mask = q_mask[:, None, None, :, None]\n",
        "\t\t\tscores_q_masked += (q_mask * -1e9)\n",
        "\n",
        "\t\tif k_mask is not None:\n",
        "\t\t\tk_mask = k_mask[:, None, None, None, :]\n",
        "\t\t\tscores_k_masked += (k_mask * -1e9)\n",
        "\n",
        "\t\tq_weights = torch.softmax(scores_k_masked, -1)\n",
        "\t\tk_weights = torch.softmax(scores_q_masked, -2)\n",
        "\t\tq_pos_weights = torch.softmax(torch.sum(scores_q_masked, dim=-1, keepdim=True), dim=-2)\n",
        "\t\tk_pos_weights = torch.softmax(torch.sum(scores_k_masked, dim=-2, keepdim=True), dim=-1)\n",
        "\n",
        "\t\tattn_out1 = torch.matmul(q_weights, wk) * q_pos_weights  # # [N, n, H, Tq, H']\n",
        "\t\tattn_out2 = torch.matmul(q_transposed, k_weights) * k_pos_weights  # # [N, n, H, H', Ta]\n",
        "\n",
        "\t\tattn_out1 = self.merge_heads(torch.sum(attn_out1, dim=-2))\n",
        "\t\tattn_out2 = self.merge_heads(torch.sum(attn_out2, dim=-1))\n",
        "\n",
        "\t\treturn (attn_out1, attn_out2)  # [N, n, H]\n"
      ],
      "metadata": {
        "id": "UdBqfN71seT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MergeAdditiveAttention(nn.Module):\n",
        "\tdef __init__(self, in_units, attention_units: int):\n",
        "\t\tsuper(MergeAdditiveAttention, self).__init__()\n",
        "\t\tself.Wq = nn.Linear(in_units, attention_units)\n",
        "\t\tself.Wk = nn.Linear(in_units, attention_units)\n",
        "\t\tself.fc = nn.Linear(attention_units, 1)\n",
        "\n",
        "\tdef _calculate_scores(self, query, key):\n",
        "\t\t\"\"\"Calculates attention scores as a nonlinear sum of query and key.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t\tquery: Query tensor of shape `[batch_size, Tq, dim]`.\n",
        "\t\t\tkey: Key tensor of shape `[batch_size, Tv, dim]`.\n",
        "\t\tReturns:\n",
        "\t\t\tTensor of shape `[batch_size, Tq, Tv]`.\n",
        "\t\t\"\"\"\n",
        "\t\t# Reshape tensors to enable broadcasting.\n",
        "\t\t# Reshape into [batch_size, Tq, 1, dim].\n",
        "\t\tq_reshaped = torch.unsqueeze(query, dim=-2)\n",
        "\t\t# Reshape into [batch_size, 1, Tv, dim].\n",
        "\t\tk_reshaped = torch.unsqueeze(key, dim=-3)\n",
        "\n",
        "\t\tx = self.fc(tf.tanh(q_reshaped + k_reshaped))  # [batch_size, Tq, Tv, S] -> [batch_size, Tq, Tv, 1]\n",
        "\t\tx = x.squeeze(-1)  # [batch_size, Tq, Tv]\n",
        "\t\treturn x\n",
        "\n",
        "\tdef forward(self, query, key):\n",
        "\t\tw_q = self.Wq(query)\n",
        "\t\tw_k = self.Wk(key)\n",
        "\t\tvalue = key\n",
        "\n",
        "\t\tscores = self._calculate_scores(w_q, w_k)\n",
        "\t\tweights = tf.softmax(scores, -1)  # [batch_size, Tq, Tv]\n",
        "\t\tcoef_weights = tf.softmax(scores, 1)\n",
        "\n",
        "\n",
        "\t\tx = torch.bmm(coef_weights, value)  # [N, Tq, Tv] x [N, Tv, S] -> [batch_size, Tq, S]\n",
        "\t\tx = torch.sum(x, dim=1)\n",
        "\t\treturn x\n"
      ],
      "metadata": {
        "id": "QUWbNaAIi7gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, feature_units, attention_units, hidden_units, num_heads, num_outputs):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.mmha = MergeMultiHeadAttention(feature_units, attention_units, num_heads)\n",
        "    self.additive_attention = MergeAdditiveAttention(feature_units, attention_units)\n",
        "    self.ffn = FeedForwardLayer(attention_units, hidden_units, feature_units)\n",
        "    self.layer_norm = nn.LayerNorm(feature_units)\n",
        "    self.ffn_out = FeedForwardLayer(feature_units, hidden_units, num_outputs)\n",
        "\n",
        "  def forward(self, q, k, q_mask, k_mask):\n",
        "    outputs = self.mmha(q, k, q_mask, k_mask)  # [N, Tq, S]\n",
        "    x = torch.cat(outputs, dim=1)  # [N, 2Tq, S]\n",
        "    x = self.ffn(x)\n",
        "    x = self.additive_attention(x, x)  # [N, S]\n",
        "    x = self.ffn_out(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "KJwj_KErcyKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = Classifier(768, 1024, 2048, 8, 2)"
      ],
      "metadata": {
        "id": "nkQluegNnXHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [N,n,Tq,H]\n",
        "a = torch.randn([2, 4, 6, 768])\n",
        "b = torch.randn([2, 4, 10, 768])\n",
        "a_mask = torch.tensor([[1, 1, 1, 1, 0, 0],\n",
        "                       [1, 1, 1, 1, 1, 1]])\n",
        "b_mask = torch.tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
        "                       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ],
      "metadata": {
        "id": "dRN2qn6ivLNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_outs = classifier(a, b, None, None)  #a_mask, b_mask)"
      ],
      "metadata": {
        "id": "0ZqaHo4Mnyr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_outs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVYhBcA0vnJB",
        "outputId": "28e033d7-c1d9-4ce9-f54c-da4554e53b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0074,  0.0095],\n",
              "        [-0.0056,  0.0122]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in classifier.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9BTDhBlvrXT",
        "outputId": "fb54257d-1a7f-49aa-97b5-1a24e757bb71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8404227"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QaModel(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super(QaModel, self).__init__()\n",
        "    self.language_model = model\n",
        "    self.classifier = Classifier(768, 1024, 2048, 8, 2)\n",
        "    # self.set_trainable(False)\n",
        "\n",
        "  def set_trainable(self, value):\n",
        "    for p in self.language_model.parameters():\n",
        "      p.requires_grad = value\n",
        "\n",
        "  def forward(self, q_inputs, t_inputs):\n",
        "    question_outputs = self.language_model(**q_inputs)\n",
        "    text_outputs = self.language_model(**t_inputs)\n",
        "    q_features = question_outputs[\"hidden_states\"][-4:]\n",
        "    t_features = text_outputs[\"hidden_states\"][-4:]\n",
        "    q_features = torch.stack(q_features, dim=1)\n",
        "    t_features = torch.stack(t_features, dim=1)\n",
        "    logits = self.classifier(q_features, t_features, q_inputs[\"attention_mask\"], t_inputs[\"attention_mask\"])\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def save(self, model_name: str, weights_only: bool = True):\n",
        "    save_path = os.path.join(\"save\", model_name)\n",
        "    # Check whether the specified path exists or not\n",
        "    is_exist = os.path.exists(save_path)\n",
        "    if not is_exist:\n",
        "      # Create a new directory because it does not exist\n",
        "      os.makedirs(save_path)\n",
        "    else:\n",
        "      print(f\"There is already a model saved with the name {model_name}, which will be overwritten by new version!\")\n",
        "    if weights_only:\n",
        "      weights_file = \"qc-weights.pt\"\n",
        "      torch.save(self.state_dict(), os.path.join(save_path, weights_file))\n",
        "\n",
        "    else:\n",
        "      model_file = \"qc-model.pt\"\n",
        "      torch.save(self, os.path.join(save_path, model_file))\n",
        "\n"
      ],
      "metadata": {
        "id": "AHngzUNcaMox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_model = QaModel(model)"
      ],
      "metadata": {
        "id": "8oH87vbj3eUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del qa_model"
      ],
      "metadata": {
        "id": "LFgKm2Zc6KFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in qa_model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PtgIDiu60Jw",
        "outputId": "d0111f7a-c40b-4d75-d55a-bf634e37d5dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "570392579"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FE7OfjG67c4",
        "outputId": "fa1bf974-eb91-404b-bbe0-99a61e0a1c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "559890432"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "9jaNmIgL2SV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QADataset(Dataset):\n",
        "    def __init__(self, tokenizer, data):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.questions = data[\"questions\"]\n",
        "        self.texts = data[\"texts\"]\n",
        "        self.labels = data[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        question_encoding = self.tokenizer(question, truncation=True)\n",
        "        text_encoding = self.tokenizer(text, truncation=True)\n",
        "\n",
        "\n",
        "        return question_encoding , text_encoding, label\n",
        "\n",
        "\n",
        "def read_qa_data(input_path):\n",
        "    labels = []\n",
        "    questions = []\n",
        "    texts = []\n",
        "\n",
        "    with open(input_path, \"r\", encoding='utf-8') as rf:\n",
        "        for line in rf:\n",
        "            question, text, label = line.strip().split(\"\\t\")\n",
        "            questions.append(question.strip())\n",
        "            texts.append(text.strip())\n",
        "            labels.append(1 if label.strip() == \"true\" else 0)\n",
        "\n",
        "    data = {\n",
        "        \"questions\": questions,\n",
        "        \"texts\": texts,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "class DataCollator():\n",
        "  def __init__(self, tokenizer):\n",
        "    self.pad_fn = DataCollatorWithPadding(tokenizer, return_tensors=\"pt\")\n",
        "\n",
        "  def __extract__(self, batch_encoding):\n",
        "    list_q = []\n",
        "    list_t = []\n",
        "    labels = []\n",
        "    for q, t, label in batch_encoding:\n",
        "      list_q.append(q)\n",
        "      list_t.append(t)\n",
        "      labels.append(label)\n",
        "\n",
        "    return list_q, list_t, labels\n",
        "\n",
        "  def __call__(self, batch_encoding):\n",
        "    q_features, t_features, labels = self.__extract__(batch_encoding)\n",
        "    q_out_padding = self.pad_fn(q_features)\n",
        "    t_out_padding = self.pad_fn(t_features)\n",
        "    labels = torch.tensor(labels)\n",
        "    return (q_out_padding, t_out_padding), labels\n",
        "\n",
        "\n",
        "def create_QA_dataset(input_path, tokenizer):\n",
        "    data = read_qa_data(input_path)\n",
        "    dataset = QADataset(tokenizer, data)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "93a_vRFn2H53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = create_QA_dataset(\"/content/train_test_origin_1k_dev.csv\",\n",
        "\t                                  tokenizer)\n",
        "val_dataset = create_QA_dataset(\"/content/val_origin_1k.csv\",\n",
        "\t                                tokenizer)"
      ],
      "metadata": {
        "id": "LkadpKhk9XAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collator = DataCollator(tokenizer)\n",
        "train_dataloader = DataLoader(train_dataset, collate_fn=collator, batch_size=8)\n",
        "val_dataloader = DataLoader(val_dataset, collate_fn=collator, batch_size=8)"
      ],
      "metadata": {
        "id": "vMS4muyY1e6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del train_dataset\n",
        "del val_dataset"
      ],
      "metadata": {
        "id": "c2armPJl4CK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del train_dataloader\n",
        "del val_dataloader"
      ],
      "metadata": {
        "id": "olacZdQI2VIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in val_dataloader:\n",
        "  break"
      ],
      "metadata": {
        "id": "jEKBRY7E2Ai7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-yAmukY4H3u",
        "outputId": "05b7577b-9269-46dc-8ef8-9db47dbaa97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 1, 0, 0, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "bTnKQrIE7BQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from torch.optim.lr_scheduler import LRScheduler"
      ],
      "metadata": {
        "id": "zmii-FlV7YzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WarmupLinearLR(LRScheduler):\n",
        "\tdef __init__(self,\n",
        "\t             optimizer,\n",
        "\t             warmup_steps,\n",
        "\t             total_steps,\n",
        "\t             min_proportion=0.0,\n",
        "\t             last_epoch=-1,\n",
        "\t             verbose=False):\n",
        "\n",
        "\t\tself.warmup_steps = warmup_steps\n",
        "\t\tself.max_steps = (total_steps - min_proportion * warmup_steps) / (1.0 - min_proportion)\n",
        "\t\tsuper(WarmupLinearLR, self).__init__(optimizer, last_epoch, verbose)\n",
        "\n",
        "\tdef get_lr(self):\n",
        "\t\tif self.last_epoch == 0:\n",
        "\t\t\treturn [group['lr'] * 0.1 / self.warmup_steps for group in self.optimizer.param_groups]\n",
        "\n",
        "\t\tif self.last_epoch > self.max_steps:\n",
        "\t\t\treturn [group['lr'] for group in self.optimizer.param_groups]\n",
        "\n",
        "\t\tif self.last_epoch < self.warmup_steps:\n",
        "\t\t\treturn [group['initial_lr'] * self.last_epoch / self.warmup_steps for group in self.optimizer.param_groups]\n",
        "\t\telse:\n",
        "\t\t\treturn [group['initial_lr'] * (self.max_steps - self.last_epoch) / (self.max_steps - self.warmup_steps) for\n",
        "\t\t\t        group in self.optimizer.param_groups]\n",
        "\n",
        "\tdef _get_closed_form_lr(self):\n",
        "\t\tif self.last_epoch < self.warmup_steps:\n",
        "\t\t\treturn [base_lr * self.last_epoch / self.warmup_steps for base_lr in self.base_lrs]\n",
        "\t\telse:\n",
        "\t\t\treturn [base_lr * (self.max_steps - self.last_epoch) / (self.max_steps - self.warmup_steps) for base_lr\n",
        "\t\t\t        in self.base_lrs]\n"
      ],
      "metadata": {
        "id": "I8qU9Z_P8LcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "gradient_accumulation_steps = 5\n",
        "steps_per_epoch = len(train_dataloader)\n",
        "total_steps = EPOCHS * (len(train_dataloader) // gradient_accumulation_steps)\n",
        "warmup_steps = int(total_steps*0.1)"
      ],
      "metadata": {
        "id": "d76X8VvI7yX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0BX-na2b7AI",
        "outputId": "b0b263f7-93e4-4ded-c75a-e1ca6489a610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8500"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warmup_steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzNzyoqVb8zd",
        "outputId": "78946a78-1403-4246-9376-c1468eb6d28a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "850"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1iC5YLqs9I6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "sNxRZwVcCplj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModel.from_pretrained(\"xlm-roberta-base\", output_hidden_states=True)"
      ],
      "metadata": {
        "id": "mX7kywVqciKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_model = QaModel(model).to(device)"
      ],
      "metadata": {
        "id": "vNLgPl7CCwV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in qa_model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP6TA5ilkFIu",
        "outputId": "c663a263-dad9-4a3e-ef88-5464a1dd93d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "286447875"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjRqrc1hkMw0",
        "outputId": "f87e16fa-6168-4e1d-88e0-b95b0968eea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "278043648"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del qa_model"
      ],
      "metadata": {
        "id": "iqFMnvP9VuE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "  {'params': [p for n, p in qa_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "    'weight_decay': 0.001},\n",
        "  {'params': [p for n, p in qa_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "    'weight_decay': 0.0}\n",
        "]"
      ],
      "metadata": {
        "id": "1Hg04MT57lG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=2e-5, weight_decay=0.0)\n",
        "scheduler = WarmupLinearLR(optimizer, warmup_steps, total_steps, min_proportion=0.0)"
      ],
      "metadata": {
        "id": "sNZU00Hz7DAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_grad_norm = 1.0\n",
        "eval_steps = 5\n",
        "save_checkpoint = True"
      ],
      "metadata": {
        "id": "6TeBPzAZDqyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_dataloader, val_steps, device):\n",
        "\tmodel.eval()\n",
        "\twith torch.no_grad():\n",
        "\t\ttotal_loss = 0.0\n",
        "\t\ty_trues = []\n",
        "\t\ty_preds = []\n",
        "\n",
        "\t\tfor batch in val_dataloader:\n",
        "\t\t\tinputs, labels = batch\n",
        "\n",
        "\t\t\tlogits = model(inputs[0].to(device), inputs[1].to(device))\n",
        "\t\t\tloss = tf.cross_entropy(logits, labels.to(device))\n",
        "\n",
        "\t\t\tpredicts = torch.argmax(logits, dim=1)\n",
        "\t\t\ty_preds.extend(predicts.cpu().numpy().tolist())\n",
        "\t\t\ty_trues.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "\t\t\ttotal_loss += loss.item()\n",
        "\n",
        "\t\tf1_micro = f1_score(y_trues, y_preds)\n",
        "\t\taccuracy = accuracy_score(y_trues, y_preds)\n",
        "\n",
        "\t\tvalidation_result = {\n",
        "\t\t\t\"loss\": round(total_loss / val_steps, 4),\n",
        "\t\t\t\"accuracy\": round(accuracy, 4),\n",
        "\t\t\t\"micro_f1\": round(f1_micro, 4)\n",
        "\t\t}\n",
        "\n",
        "\treturn validation_result\n"
      ],
      "metadata": {
        "id": "ZE8u9xXrFvou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIVTgM-RUbeI",
        "outputId": "b5bf5128-62a3-4cdf-c7c2-ceebd68c07e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_writer = open(\"/content/logs/train-qc.log\", \"w\")\n",
        "log_writer.write(\"               ***** Start training *****\\n\")\n",
        "log_writer.write(\"============================================================\\n\")\n",
        "log_writer.write(f\"Num samples: {len(train_dataset)}\\n\")\n",
        "log_writer.write(f\"Num epochs: {EPOCHS}\\n\")\n",
        "log_writer.write(f\"Gradient accumulation steps = {gradient_accumulation_steps}\\n\")\n",
        "log_writer.write(\"============================================================\\n\")\n",
        "\n",
        "monitor_f1 = float('-inf')\n",
        "\n",
        "# qa_model.set_trainable(False)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  total_loss = 0.0\n",
        "  y_trues = []\n",
        "  y_preds = []\n",
        "\n",
        "  log_writer.write(\"------------------------------------------------------------\\n\")\n",
        "  log_writer.write(f\"Epoch {epoch + 1:>3d}/{EPOCHS}:\\n\")\n",
        "\n",
        "  global_steps = 0\n",
        "\n",
        "  print(f\"Epoch \\033[92m{epoch + 1:>3d}/{EPOCHS}\\033[00m:\")\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    print(f\"\\r- Step \\033[96m{step + 1:>5d}/{steps_per_epoch}\\033[00m:\", end=\"\")\n",
        "\n",
        "    qa_model.train()\n",
        "\n",
        "    inputs, labels = batch\n",
        "\n",
        "    logits = qa_model(inputs[0].to(device), inputs[1].to(device))\n",
        "    loss = tf.cross_entropy(logits, labels.to(device))\n",
        "\n",
        "    predicts = torch.argmax(logits, dim=1)\n",
        "    y_preds.extend(predicts.cpu().numpy().tolist())\n",
        "    y_trues.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    loss /= gradient_accumulation_steps\n",
        "    loss.backward()\n",
        "\n",
        "    if (step + 1) % gradient_accumulation_steps == 0 or (step == steps_per_epoch - 1):\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()  # Update learning rate schedule\n",
        "      optimizer.zero_grad()\n",
        "      global_steps += 1\n",
        "      # if global_steps == 50:\n",
        "      #   log_writer.write(\"Fully training\\n\")\n",
        "      #   qa_model.set_trainable(True)\n",
        "\n",
        "      if global_steps % eval_steps == 0 or (step == steps_per_epoch - 1):\n",
        "        print()\n",
        "        logging_line = f\"- Step: {step + 1:>5d}/{steps_per_epoch}, lr: {scheduler.get_last_lr()}\\n\"\n",
        "        log_writer.write(logging_line)\n",
        "        # print(y_trues)\n",
        "        # print(y_preds)\n",
        "        f1_micro = f1_score(y_trues, y_preds)\n",
        "        accuracy = accuracy_score(y_trues, y_preds)\n",
        "\n",
        "        train_accumulate_loss = round(total_loss / (step + 1), 4)\n",
        "        train_accumulate_micro_f1 = round(f1_micro, 4)\n",
        "        train_accumulate_accuracy = round(accuracy, 4)\n",
        "\n",
        "        train_result_line = (f\"{'loss':8s}: {train_accumulate_loss:<10.4f} - \"\n",
        "                              f\"{'accuracy':12s}: {train_accumulate_accuracy:<10.4f} - \"\n",
        "                              f\"{'f1':12s}: {train_accumulate_micro_f1:<10.4f}\")\n",
        "\n",
        "        print(f\"    \\033[95m{'Train result':20s}\\033[00m - {train_result_line}\")\n",
        "        log_writer.write(f\"    {'Train result':20s} - {train_result_line}\\n\")\n",
        "\n",
        "        validation_output = evaluate(qa_model, val_dataloader, len(val_dataloader), device)\n",
        "\n",
        "        val_result_line = (f\"val_loss: {validation_output['loss']:<10.4f} - \"\n",
        "                            f\"val_accuracy: {validation_output['accuracy']:<10.4f} - \"\n",
        "                            f\"val_f1: {validation_output['micro_f1']:<10.4f}\")\n",
        "\n",
        "        print(f\"    \\033[95m{'Validation result':20s}\\033[00m - {val_result_line}\")\n",
        "        log_writer.write(f\"    {'Validation result':20s} - {val_result_line}\\n\")\n",
        "        if save_checkpoint:\n",
        "          if validation_output['micro_f1'] > monitor_f1:\n",
        "            qa_model.save(\"xlm-roberta-large-qa\")\n",
        "            log_writer.write(\n",
        "              f\"    # val_f1 improve from {monitor_f1} to {validation_output['micro_f1']}. \"\n",
        "              \"Saving model with name \\\"xlm-roberta-large-qa\\\"\")\n",
        "            monitor_f1 = validation_output[\"micro_f1\"]\n",
        "\n",
        "        log_writer.write(\"\\n\")\n",
        "\n",
        "log_writer.write(\"                ***** End training *****\\n\")\n",
        "log_writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai_zGAX48j-A",
        "outputId": "365cbf3c-1c2f-42a0-e564-0b7bdbb3240a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch \u001b[92m  1/20\u001b[00m:\n",
            "- Step \u001b[96m   25/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.7005     - accuracy    : 0.4100     - f1          : 0.4381    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7006     - val_accuracy: 0.3707     - val_f1: 0.4628    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m   50/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.7010     - accuracy    : 0.4000     - f1          : 0.4231    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6980     - val_accuracy: 0.3716     - val_f1: 0.4632    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m   75/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6981     - accuracy    : 0.4383     - f1          : 0.4556    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6941     - val_accuracy: 0.3896     - val_f1: 0.4534    \n",
            "- Step \u001b[96m  100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6970     - accuracy    : 0.4462     - f1          : 0.4483    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6894     - val_accuracy: 0.4883     - val_f1: 0.4498    \n",
            "- Step \u001b[96m  125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6955     - accuracy    : 0.4620     - f1          : 0.4227    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6831     - val_accuracy: 0.6777     - val_f1: 0.0477    \n",
            "- Step \u001b[96m  150/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6925     - accuracy    : 0.4967     - f1          : 0.4113    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6754     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  175/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6884     - accuracy    : 0.5336     - f1          : 0.3937    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6663     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  200/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6856     - accuracy    : 0.5538     - f1          : 0.3726    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6568     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  225/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6820     - accuracy    : 0.5694     - f1          : 0.3536    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6483     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  250/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6793     - accuracy    : 0.5790     - f1          : 0.3349    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6415     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  275/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6746     - accuracy    : 0.5923     - f1          : 0.3210    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6349     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  300/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6732     - accuracy    : 0.5958     - f1          : 0.3042    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6301     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  325/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6682     - accuracy    : 0.6062     - f1          : 0.2928    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6264     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  350/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6665     - accuracy    : 0.6096     - f1          : 0.2795    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6239     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  375/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6628     - accuracy    : 0.6163     - f1          : 0.2692    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6223     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  400/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6589     - accuracy    : 0.6225     - f1          : 0.2598    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6216     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  425/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6543     - accuracy    : 0.6294     - f1          : 0.2518    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6224     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  450/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6503     - accuracy    : 0.6350     - f1          : 0.2440    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6239     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  475/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6496     - accuracy    : 0.6363     - f1          : 0.2348    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6267     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  500/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6504     - accuracy    : 0.6362     - f1          : 0.2257    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6277     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  525/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6473     - accuracy    : 0.6405     - f1          : 0.2192    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6272     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  550/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6447     - accuracy    : 0.6443     - f1          : 0.2132    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6279     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  575/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6443     - accuracy    : 0.6452     - f1          : 0.2062    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6265     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  600/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6418     - accuracy    : 0.6485     - f1          : 0.2009    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6249     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  625/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6411     - accuracy    : 0.6498     - f1          : 0.1949    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6239     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  650/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6401     - accuracy    : 0.6513     - f1          : 0.1895    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6228     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  675/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6416     - accuracy    : 0.6496     - f1          : 0.1831    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6214     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  700/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6418     - accuracy    : 0.6496     - f1          : 0.1777    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6239     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  725/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6409     - accuracy    : 0.6509     - f1          : 0.1731    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6240     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  750/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6393     - accuracy    : 0.6532     - f1          : 0.1693    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6220     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  775/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6387     - accuracy    : 0.6539     - f1          : 0.1650    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6178     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  800/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6391     - accuracy    : 0.6531     - f1          : 0.1604    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6162     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  825/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6386     - accuracy    : 0.6536     - f1          : 0.1565    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6137     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  850/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6380     - accuracy    : 0.6543     - f1          : 0.1528    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6147     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  875/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6370     - accuracy    : 0.6559     - f1          : 0.1497    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6115     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  900/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6369     - accuracy    : 0.6556     - f1          : 0.1460    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6075     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  925/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6351     - accuracy    : 0.6576     - f1          : 0.1433    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6044     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  950/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6340     - accuracy    : 0.6586     - f1          : 0.1404    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6025     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m  975/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6333     - accuracy    : 0.6590     - f1          : 0.1375    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6011     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m 1000/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6328     - accuracy    : 0.6592     - f1          : 0.1346    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6005     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m 1025/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6318     - accuracy    : 0.6598     - f1          : 0.1319    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6014     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m 1050/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6324     - accuracy    : 0.6588     - f1          : 0.1289    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6139     - val_accuracy: 0.6930     - val_f1: 0.1934    \n",
            "- Step \u001b[96m 1075/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6322     - accuracy    : 0.6588     - f1          : 0.1314    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6261     - val_accuracy: 0.6741     - val_f1: 0.3960    \n",
            "- Step \u001b[96m 1100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6327     - accuracy    : 0.6584     - f1          : 0.1402    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6366     - val_accuracy: 0.6364     - val_f1: 0.4414    \n",
            "- Step \u001b[96m 1125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6317     - accuracy    : 0.6592     - f1          : 0.1426    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6063     - val_accuracy: 0.6813     - val_f1: 0.0731    \n",
            "- Step \u001b[96m 1150/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6298     - accuracy    : 0.6611     - f1          : 0.1410    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6019     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m 1175/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6308     - accuracy    : 0.6599     - f1          : 0.1380    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5994     - val_accuracy: 0.6831     - val_f1: 0.0000    \n",
            "- Step \u001b[96m 1200/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6316     - accuracy    : 0.6589     - f1          : 0.1357    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5967     - val_accuracy: 0.6876     - val_f1: 0.1792    \n",
            "- Step \u001b[96m 1225/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6312     - accuracy    : 0.6590     - f1          : 0.1369    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6105     - val_accuracy: 0.6759     - val_f1: 0.4455    \n",
            "- Step \u001b[96m 1250/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6304     - accuracy    : 0.6597     - f1          : 0.1396    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5988     - val_accuracy: 0.6957     - val_f1: 0.4042    \n",
            "- Step \u001b[96m 1275/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6291     - accuracy    : 0.6605     - f1          : 0.1401    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5856     - val_accuracy: 0.6894     - val_f1: 0.2511    \n",
            "- Step \u001b[96m 1300/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6276     - accuracy    : 0.6617     - f1          : 0.1390    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5835     - val_accuracy: 0.6894     - val_f1: 0.1602    \n",
            "- Step \u001b[96m 1325/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6272     - accuracy    : 0.6617     - f1          : 0.1372    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5838     - val_accuracy: 0.6966     - val_f1: 0.2248    \n",
            "- Step \u001b[96m 1350/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6260     - accuracy    : 0.6632     - f1          : 0.1379    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5847     - val_accuracy: 0.6939     - val_f1: 0.2821    \n",
            "- Step \u001b[96m 1375/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6248     - accuracy    : 0.6635     - f1          : 0.1391    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5887     - val_accuracy: 0.6966     - val_f1: 0.4348    \n",
            "- Step \u001b[96m 1400/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6247     - accuracy    : 0.6637     - f1          : 0.1425    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6088     - val_accuracy: 0.6481     - val_f1: 0.4856    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m 1425/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6243     - accuracy    : 0.6644     - f1          : 0.1460    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5898     - val_accuracy: 0.6957     - val_f1: 0.4523    \n",
            "- Step \u001b[96m 1450/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6228     - accuracy    : 0.6657     - f1          : 0.1473    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5839     - val_accuracy: 0.6948     - val_f1: 0.2130    \n",
            "- Step \u001b[96m 1475/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6226     - accuracy    : 0.6657     - f1          : 0.1455    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5834     - val_accuracy: 0.6921     - val_f1: 0.2899    \n",
            "- Step \u001b[96m 1500/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6215     - accuracy    : 0.6668     - f1          : 0.1461    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5909     - val_accuracy: 0.6894     - val_f1: 0.3447    \n",
            "- Step \u001b[96m 1525/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6214     - accuracy    : 0.6670     - f1          : 0.1477    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5909     - val_accuracy: 0.6957     - val_f1: 0.3179    \n",
            "- Step \u001b[96m 1550/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6210     - accuracy    : 0.6673     - f1          : 0.1479    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5946     - val_accuracy: 0.6876     - val_f1: 0.4441    \n",
            "- Step \u001b[96m 1575/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6201     - accuracy    : 0.6681     - f1          : 0.1507    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5854     - val_accuracy: 0.6885     - val_f1: 0.4109    \n",
            "- Step \u001b[96m 1600/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6188     - accuracy    : 0.6690     - f1          : 0.1524    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5777     - val_accuracy: 0.6984     - val_f1: 0.4105    \n",
            "- Step \u001b[96m 1625/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6180     - accuracy    : 0.6704     - f1          : 0.1540    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5759     - val_accuracy: 0.6966     - val_f1: 0.4049    \n",
            "- Step \u001b[96m 1650/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6169     - accuracy    : 0.6712     - f1          : 0.1573    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5780     - val_accuracy: 0.6966     - val_f1: 0.4940    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m 1675/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6168     - accuracy    : 0.6715     - f1          : 0.1631    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6018     - val_accuracy: 0.6526     - val_f1: 0.5505    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m 1700/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6162     - accuracy    : 0.6713     - f1          : 0.1685    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5777     - val_accuracy: 0.6975     - val_f1: 0.5313    \n",
            "- Step \u001b[96m 1725/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6161     - accuracy    : 0.6713     - f1          : 0.1720    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5727     - val_accuracy: 0.7002     - val_f1: 0.5269    \n",
            "- Step \u001b[96m 1750/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6150     - accuracy    : 0.6721     - f1          : 0.1762    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5606     - val_accuracy: 0.7145     - val_f1: 0.5152    \n",
            "- Step \u001b[96m 1775/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6140     - accuracy    : 0.6725     - f1          : 0.1787    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5550     - val_accuracy: 0.7235     - val_f1: 0.5172    \n",
            "- Step \u001b[96m 1800/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6137     - accuracy    : 0.6726     - f1          : 0.1824    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5929     - val_accuracy: 0.6625     - val_f1: 0.5678    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m 1825/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6129     - accuracy    : 0.6734     - f1          : 0.1899    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5633     - val_accuracy: 0.6984     - val_f1: 0.5116    \n",
            "- Step \u001b[96m 1850/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6116     - accuracy    : 0.6741     - f1          : 0.1925    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5612     - val_accuracy: 0.7118     - val_f1: 0.4298    \n",
            "- Step \u001b[96m 1875/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6113     - accuracy    : 0.6741     - f1          : 0.1958    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5742     - val_accuracy: 0.6912     - val_f1: 0.5235    \n",
            "- Step \u001b[96m 1900/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6102     - accuracy    : 0.6751     - f1          : 0.2033    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5738     - val_accuracy: 0.6948     - val_f1: 0.5291    \n",
            "- Step \u001b[96m 1925/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6101     - accuracy    : 0.6758     - f1          : 0.2071    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5616     - val_accuracy: 0.7092     - val_f1: 0.3955    \n",
            "- Step \u001b[96m 1950/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6092     - accuracy    : 0.6763     - f1          : 0.2085    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5530     - val_accuracy: 0.7110     - val_f1: 0.4633    \n",
            "- Step \u001b[96m 1975/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6083     - accuracy    : 0.6773     - f1          : 0.2108    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5477     - val_accuracy: 0.7208     - val_f1: 0.4054    \n",
            "- Step \u001b[96m 2000/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6080     - accuracy    : 0.6776     - f1          : 0.2137    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5619     - val_accuracy: 0.7101     - val_f1: 0.5557    \n",
            "- Step \u001b[96m 2025/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6075     - accuracy    : 0.6782     - f1          : 0.2174    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5482     - val_accuracy: 0.7172     - val_f1: 0.4960    \n",
            "- Step \u001b[96m 2050/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6072     - accuracy    : 0.6782     - f1          : 0.2200    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5431     - val_accuracy: 0.7307     - val_f1: 0.4966    \n",
            "- Step \u001b[96m 2075/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6066     - accuracy    : 0.6786     - f1          : 0.2224    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5519     - val_accuracy: 0.7118     - val_f1: 0.5355    \n",
            "- Step \u001b[96m 2100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6057     - accuracy    : 0.6792     - f1          : 0.2254    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5435     - val_accuracy: 0.7253     - val_f1: 0.4706    \n",
            "- Step \u001b[96m 2125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.6046     - accuracy    : 0.6801     - f1          : 0.2296    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5532     - val_accuracy: 0.7074     - val_f1: 0.5134    \n",
            "Epoch \u001b[92m  2/20\u001b[00m:\n",
            "- Step \u001b[96m   25/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5151     - accuracy    : 0.7400     - f1          : 0.5273    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5746     - val_accuracy: 0.6822     - val_f1: 0.5216    \n",
            "- Step \u001b[96m   50/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5040     - accuracy    : 0.7600     - f1          : 0.5102    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5718     - val_accuracy: 0.7145     - val_f1: 0.3977    \n",
            "- Step \u001b[96m   75/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5296     - accuracy    : 0.7467     - f1          : 0.5159    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5731     - val_accuracy: 0.7047     - val_f1: 0.5665    \n",
            "- Step \u001b[96m  100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5434     - accuracy    : 0.7312     - f1          : 0.5376    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5441     - val_accuracy: 0.7181     - val_f1: 0.4767    \n",
            "- Step \u001b[96m  125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5446     - accuracy    : 0.7260     - f1          : 0.4963    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5542     - val_accuracy: 0.7011     - val_f1: 0.1696    \n",
            "- Step \u001b[96m  150/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5469     - accuracy    : 0.7242     - f1          : 0.4635    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5347     - val_accuracy: 0.7298     - val_f1: 0.4477    \n",
            "- Step \u001b[96m  175/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5352     - accuracy    : 0.7364     - f1          : 0.4644    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5500     - val_accuracy: 0.7118     - val_f1: 0.2654    \n",
            "- Step \u001b[96m  200/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5432     - accuracy    : 0.7338     - f1          : 0.4350    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5687     - val_accuracy: 0.7127     - val_f1: 0.2694    \n",
            "- Step \u001b[96m  225/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5424     - accuracy    : 0.7322     - f1          : 0.4289    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5702     - val_accuracy: 0.6975     - val_f1: 0.5983    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m  250/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5499     - accuracy    : 0.7220     - f1          : 0.4429    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5480     - val_accuracy: 0.7127     - val_f1: 0.5789    \n",
            "- Step \u001b[96m  275/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5476     - accuracy    : 0.7236     - f1          : 0.4432    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5730     - val_accuracy: 0.6939     - val_f1: 0.1097    \n",
            "- Step \u001b[96m  300/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5538     - accuracy    : 0.7183     - f1          : 0.4242    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5597     - val_accuracy: 0.6930     - val_f1: 0.1140    \n",
            "- Step \u001b[96m  325/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5517     - accuracy    : 0.7192     - f1          : 0.4169    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5388     - val_accuracy: 0.7217     - val_f1: 0.3922    \n",
            "- Step \u001b[96m  350/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5504     - accuracy    : 0.7196     - f1          : 0.4266    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5379     - val_accuracy: 0.7190     - val_f1: 0.4668    \n",
            "- Step \u001b[96m  375/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5496     - accuracy    : 0.7197     - f1          : 0.4314    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5371     - val_accuracy: 0.7253     - val_f1: 0.3598    \n",
            "- Step \u001b[96m  400/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5486     - accuracy    : 0.7194     - f1          : 0.4244    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5618     - val_accuracy: 0.7029     - val_f1: 0.2062    \n",
            "- Step \u001b[96m  425/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5456     - accuracy    : 0.7224     - f1          : 0.4180    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5499     - val_accuracy: 0.7199     - val_f1: 0.3004    \n",
            "- Step \u001b[96m  450/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5475     - accuracy    : 0.7219     - f1          : 0.4094    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5415     - val_accuracy: 0.7181     - val_f1: 0.5590    \n",
            "- Step \u001b[96m  475/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5451     - accuracy    : 0.7255     - f1          : 0.4247    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5472     - val_accuracy: 0.7101     - val_f1: 0.5699    \n",
            "- Step \u001b[96m  500/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5451     - accuracy    : 0.7260     - f1          : 0.4374    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5333     - val_accuracy: 0.7190     - val_f1: 0.5706    \n",
            "- Step \u001b[96m  525/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5403     - accuracy    : 0.7307     - f1          : 0.4480    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5327     - val_accuracy: 0.7199     - val_f1: 0.5581    \n",
            "- Step \u001b[96m  550/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5403     - accuracy    : 0.7305     - f1          : 0.4458    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5369     - val_accuracy: 0.7370     - val_f1: 0.5042    \n",
            "- Step \u001b[96m  575/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5415     - accuracy    : 0.7293     - f1          : 0.4459    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5489     - val_accuracy: 0.7101     - val_f1: 0.5885    \n",
            "- Step \u001b[96m  600/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5395     - accuracy    : 0.7315     - f1          : 0.4531    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5232     - val_accuracy: 0.7415     - val_f1: 0.4607    \n",
            "- Step \u001b[96m  625/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5409     - accuracy    : 0.7300     - f1          : 0.4476    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5356     - val_accuracy: 0.7172     - val_f1: 0.2825    \n",
            "- Step \u001b[96m  650/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5393     - accuracy    : 0.7308     - f1          : 0.4479    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5310     - val_accuracy: 0.7244     - val_f1: 0.5072    \n",
            "- Step \u001b[96m  675/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5403     - accuracy    : 0.7294     - f1          : 0.4551    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5541     - val_accuracy: 0.7163     - val_f1: 0.5831    \n",
            "- Step \u001b[96m  700/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5394     - accuracy    : 0.7295     - f1          : 0.4591    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5394     - val_accuracy: 0.7172     - val_f1: 0.5506    \n",
            "- Step \u001b[96m  725/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5392     - accuracy    : 0.7293     - f1          : 0.4601    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5355     - val_accuracy: 0.7334     - val_f1: 0.4975    \n",
            "- Step \u001b[96m  750/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5376     - accuracy    : 0.7298     - f1          : 0.4613    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5328     - val_accuracy: 0.7298     - val_f1: 0.5432    \n",
            "- Step \u001b[96m  775/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5384     - accuracy    : 0.7302     - f1          : 0.4643    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5280     - val_accuracy: 0.7370     - val_f1: 0.5092    \n",
            "- Step \u001b[96m  800/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5392     - accuracy    : 0.7298     - f1          : 0.4669    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5312     - val_accuracy: 0.7226     - val_f1: 0.5239    \n",
            "- Step \u001b[96m  825/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5382     - accuracy    : 0.7314     - f1          : 0.4709    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5302     - val_accuracy: 0.7370     - val_f1: 0.5189    \n",
            "- Step \u001b[96m  850/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5385     - accuracy    : 0.7315     - f1          : 0.4716    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5303     - val_accuracy: 0.7298     - val_f1: 0.5554    \n",
            "- Step \u001b[96m  875/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5374     - accuracy    : 0.7317     - f1          : 0.4722    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5227     - val_accuracy: 0.7442     - val_f1: 0.5009    \n",
            "- Step \u001b[96m  900/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5381     - accuracy    : 0.7310     - f1          : 0.4706    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5255     - val_accuracy: 0.7370     - val_f1: 0.5891    \n",
            "- Step \u001b[96m  925/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5370     - accuracy    : 0.7319     - f1          : 0.4740    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5266     - val_accuracy: 0.7316     - val_f1: 0.5876    \n",
            "- Step \u001b[96m  950/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5362     - accuracy    : 0.7329     - f1          : 0.4771    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5235     - val_accuracy: 0.7388     - val_f1: 0.4561    \n",
            "- Step \u001b[96m  975/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5369     - accuracy    : 0.7327     - f1          : 0.4765    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5182     - val_accuracy: 0.7415     - val_f1: 0.4686    \n",
            "- Step \u001b[96m 1000/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5366     - accuracy    : 0.7329     - f1          : 0.4769    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5246     - val_accuracy: 0.7235     - val_f1: 0.5457    \n",
            "- Step \u001b[96m 1025/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5367     - accuracy    : 0.7322     - f1          : 0.4786    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5235     - val_accuracy: 0.7307     - val_f1: 0.4792    \n",
            "- Step \u001b[96m 1050/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5382     - accuracy    : 0.7311     - f1          : 0.4789    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5283     - val_accuracy: 0.7316     - val_f1: 0.5544    \n",
            "- Step \u001b[96m 1075/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5380     - accuracy    : 0.7320     - f1          : 0.4840    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5269     - val_accuracy: 0.7361     - val_f1: 0.5333    \n",
            "- Step \u001b[96m 1100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5383     - accuracy    : 0.7316     - f1          : 0.4870    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5183     - val_accuracy: 0.7325     - val_f1: 0.5387    \n",
            "- Step \u001b[96m 1125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5376     - accuracy    : 0.7321     - f1          : 0.4863    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5355     - val_accuracy: 0.7262     - val_f1: 0.3659    \n",
            "- Step \u001b[96m 1150/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5359     - accuracy    : 0.7329     - f1          : 0.4837    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5426     - val_accuracy: 0.7334     - val_f1: 0.4095    \n",
            "- Step \u001b[96m 1175/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5368     - accuracy    : 0.7313     - f1          : 0.4828    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5620     - val_accuracy: 0.7056     - val_f1: 0.6095    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m 1200/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5368     - accuracy    : 0.7319     - f1          : 0.4915    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5742     - val_accuracy: 0.6984     - val_f1: 0.6190    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m 1225/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5365     - accuracy    : 0.7318     - f1          : 0.4936    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5152     - val_accuracy: 0.7388     - val_f1: 0.4436    \n",
            "- Step \u001b[96m 1250/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5360     - accuracy    : 0.7319     - f1          : 0.4910    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5015     - val_accuracy: 0.7496     - val_f1: 0.5388    \n",
            "- Step \u001b[96m 1275/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5351     - accuracy    : 0.7325     - f1          : 0.4930    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5236     - val_accuracy: 0.7379     - val_f1: 0.6266    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m 1300/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5347     - accuracy    : 0.7328     - f1          : 0.4948    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5121     - val_accuracy: 0.7522     - val_f1: 0.5563    \n",
            "- Step \u001b[96m 1325/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5343     - accuracy    : 0.7330     - f1          : 0.4943    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5158     - val_accuracy: 0.7504     - val_f1: 0.5531    \n",
            "- Step \u001b[96m 1350/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5335     - accuracy    : 0.7338     - f1          : 0.4950    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5135     - val_accuracy: 0.7478     - val_f1: 0.5787    \n",
            "- Step \u001b[96m 1375/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5323     - accuracy    : 0.7351     - f1          : 0.4988    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5123     - val_accuracy: 0.7487     - val_f1: 0.5977    \n",
            "- Step \u001b[96m 1400/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5328     - accuracy    : 0.7350     - f1          : 0.4997    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5098     - val_accuracy: 0.7549     - val_f1: 0.5956    \n",
            "- Step \u001b[96m 1425/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5333     - accuracy    : 0.7347     - f1          : 0.4982    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5092     - val_accuracy: 0.7540     - val_f1: 0.5785    \n",
            "- Step \u001b[96m 1450/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5318     - accuracy    : 0.7359     - f1          : 0.4993    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5154     - val_accuracy: 0.7451     - val_f1: 0.5154    \n",
            "- Step \u001b[96m 1475/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5318     - accuracy    : 0.7358     - f1          : 0.4991    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5100     - val_accuracy: 0.7469     - val_f1: 0.5607    \n",
            "- Step \u001b[96m 1500/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5310     - accuracy    : 0.7362     - f1          : 0.4987    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5144     - val_accuracy: 0.7469     - val_f1: 0.5268    \n",
            "- Step \u001b[96m 1525/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5320     - accuracy    : 0.7353     - f1          : 0.4981    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5176     - val_accuracy: 0.7316     - val_f1: 0.6172    \n",
            "- Step \u001b[96m 1550/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5331     - accuracy    : 0.7345     - f1          : 0.4988    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5253     - val_accuracy: 0.7352     - val_f1: 0.6093    \n",
            "- Step \u001b[96m 1575/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5325     - accuracy    : 0.7349     - f1          : 0.4994    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5176     - val_accuracy: 0.7307     - val_f1: 0.4118    \n",
            "- Step \u001b[96m 1600/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5313     - accuracy    : 0.7358     - f1          : 0.4997    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5012     - val_accuracy: 0.7567     - val_f1: 0.6009    \n",
            "- Step \u001b[96m 1625/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5308     - accuracy    : 0.7360     - f1          : 0.4997    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5108     - val_accuracy: 0.7469     - val_f1: 0.6338    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m 1650/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5298     - accuracy    : 0.7363     - f1          : 0.5012    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5029     - val_accuracy: 0.7585     - val_f1: 0.5737    \n",
            "- Step \u001b[96m 1675/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5300     - accuracy    : 0.7360     - f1          : 0.5018    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5059     - val_accuracy: 0.7451     - val_f1: 0.6110    \n",
            "- Step \u001b[96m 1700/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5299     - accuracy    : 0.7360     - f1          : 0.5028    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4975     - val_accuracy: 0.7540     - val_f1: 0.5935    \n",
            "- Step \u001b[96m 1725/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5298     - accuracy    : 0.7358     - f1          : 0.5035    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4943     - val_accuracy: 0.7469     - val_f1: 0.5766    \n",
            "- Step \u001b[96m 1750/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5285     - accuracy    : 0.7365     - f1          : 0.5054    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4943     - val_accuracy: 0.7648     - val_f1: 0.5774    \n",
            "- Step \u001b[96m 1775/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5277     - accuracy    : 0.7373     - f1          : 0.5070    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4953     - val_accuracy: 0.7621     - val_f1: 0.6074    \n",
            "- Step \u001b[96m 1800/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5281     - accuracy    : 0.7371     - f1          : 0.5077    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5339     - val_accuracy: 0.7190     - val_f1: 0.6296    \n",
            "- Step \u001b[96m 1825/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5273     - accuracy    : 0.7379     - f1          : 0.5111    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5052     - val_accuracy: 0.7487     - val_f1: 0.5942    \n",
            "- Step \u001b[96m 1850/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5259     - accuracy    : 0.7389     - f1          : 0.5124    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5297     - val_accuracy: 0.7504     - val_f1: 0.5123    \n",
            "- Step \u001b[96m 1875/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5259     - accuracy    : 0.7389     - f1          : 0.5129    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5546     - val_accuracy: 0.7127     - val_f1: 0.6253    \n",
            "- Step \u001b[96m 1900/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5258     - accuracy    : 0.7388     - f1          : 0.5156    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5114     - val_accuracy: 0.7424     - val_f1: 0.5859    \n",
            "- Step \u001b[96m 1925/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5259     - accuracy    : 0.7388     - f1          : 0.5151    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5224     - val_accuracy: 0.7316     - val_f1: 0.3984    \n",
            "- Step \u001b[96m 1950/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5256     - accuracy    : 0.7390     - f1          : 0.5139    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4964     - val_accuracy: 0.7540     - val_f1: 0.5324    \n",
            "- Step \u001b[96m 1975/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5251     - accuracy    : 0.7393     - f1          : 0.5139    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4910     - val_accuracy: 0.7702     - val_f1: 0.6179    \n",
            "- Step \u001b[96m 2000/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5249     - accuracy    : 0.7395     - f1          : 0.5151    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4918     - val_accuracy: 0.7702     - val_f1: 0.6444    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m 2025/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5248     - accuracy    : 0.7394     - f1          : 0.5145    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4921     - val_accuracy: 0.7540     - val_f1: 0.5243    \n",
            "- Step \u001b[96m 2050/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5246     - accuracy    : 0.7394     - f1          : 0.5153    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5081     - val_accuracy: 0.7513     - val_f1: 0.6426    \n",
            "- Step \u001b[96m 2075/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5249     - accuracy    : 0.7390     - f1          : 0.5159    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5084     - val_accuracy: 0.7496     - val_f1: 0.6194    \n",
            "- Step \u001b[96m 2100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5243     - accuracy    : 0.7393     - f1          : 0.5162    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5068     - val_accuracy: 0.7469     - val_f1: 0.4514    \n",
            "- Step \u001b[96m 2125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.5230     - accuracy    : 0.7401     - f1          : 0.5172    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5012     - val_accuracy: 0.7504     - val_f1: 0.6040    \n",
            "Epoch \u001b[92m  3/20\u001b[00m:\n",
            "- Step \u001b[96m   25/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4657     - accuracy    : 0.7650     - f1          : 0.6179    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5709     - val_accuracy: 0.7029     - val_f1: 0.6092    \n",
            "- Step \u001b[96m   50/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4340     - accuracy    : 0.7850     - f1          : 0.6293    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5624     - val_accuracy: 0.7478     - val_f1: 0.5603    \n",
            "- Step \u001b[96m   75/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4715     - accuracy    : 0.7733     - f1          : 0.6158    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5305     - val_accuracy: 0.7451     - val_f1: 0.5860    \n",
            "- Step \u001b[96m  100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4853     - accuracy    : 0.7600     - f1          : 0.6145    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5599     - val_accuracy: 0.6975     - val_f1: 0.6201    \n",
            "- Step \u001b[96m  125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4875     - accuracy    : 0.7630     - f1          : 0.6220    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5021     - val_accuracy: 0.7558     - val_f1: 0.5374    \n",
            "- Step \u001b[96m  150/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4885     - accuracy    : 0.7600     - f1          : 0.5978    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5065     - val_accuracy: 0.7504     - val_f1: 0.5174    \n",
            "- Step \u001b[96m  175/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4802     - accuracy    : 0.7686     - f1          : 0.5930    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5123     - val_accuracy: 0.7478     - val_f1: 0.5355    \n",
            "- Step \u001b[96m  200/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4854     - accuracy    : 0.7656     - f1          : 0.5743    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5043     - val_accuracy: 0.7576     - val_f1: 0.5921    \n",
            "- Step \u001b[96m  225/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4808     - accuracy    : 0.7689     - f1          : 0.5823    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5111     - val_accuracy: 0.7334     - val_f1: 0.6107    \n",
            "- Step \u001b[96m  250/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4823     - accuracy    : 0.7680     - f1          : 0.5865    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5026     - val_accuracy: 0.7406     - val_f1: 0.5781    \n",
            "- Step \u001b[96m  275/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4817     - accuracy    : 0.7686     - f1          : 0.5818    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5141     - val_accuracy: 0.7442     - val_f1: 0.5366    \n",
            "- Step \u001b[96m  300/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4844     - accuracy    : 0.7671     - f1          : 0.5800    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5265     - val_accuracy: 0.7325     - val_f1: 0.5373    \n",
            "- Step \u001b[96m  325/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4843     - accuracy    : 0.7681     - f1          : 0.5792    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5268     - val_accuracy: 0.7244     - val_f1: 0.5411    \n",
            "- Step \u001b[96m  350/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4858     - accuracy    : 0.7671     - f1          : 0.5783    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5164     - val_accuracy: 0.7334     - val_f1: 0.5395    \n",
            "- Step \u001b[96m  375/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4876     - accuracy    : 0.7673     - f1          : 0.5770    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5121     - val_accuracy: 0.7343     - val_f1: 0.5418    \n",
            "- Step \u001b[96m  400/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4860     - accuracy    : 0.7666     - f1          : 0.5729    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5266     - val_accuracy: 0.7451     - val_f1: 0.4701    \n",
            "- Step \u001b[96m  425/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4838     - accuracy    : 0.7688     - f1          : 0.5691    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5180     - val_accuracy: 0.7406     - val_f1: 0.5175    \n",
            "- Step \u001b[96m  450/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4873     - accuracy    : 0.7656     - f1          : 0.5600    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5353     - val_accuracy: 0.7361     - val_f1: 0.6260    \n",
            "- Step \u001b[96m  475/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4844     - accuracy    : 0.7689     - f1          : 0.5696    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5118     - val_accuracy: 0.7352     - val_f1: 0.5551    \n",
            "- Step \u001b[96m  500/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4856     - accuracy    : 0.7672     - f1          : 0.5684    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5151     - val_accuracy: 0.7478     - val_f1: 0.6081    \n",
            "- Step \u001b[96m  525/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4823     - accuracy    : 0.7698     - f1          : 0.5734    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5273     - val_accuracy: 0.7433     - val_f1: 0.6266    \n",
            "- Step \u001b[96m  550/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4818     - accuracy    : 0.7700     - f1          : 0.5737    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5303     - val_accuracy: 0.7504     - val_f1: 0.5190    \n",
            "- Step \u001b[96m  575/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4808     - accuracy    : 0.7700     - f1          : 0.5730    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5151     - val_accuracy: 0.7415     - val_f1: 0.6129    \n",
            "- Step \u001b[96m  600/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4781     - accuracy    : 0.7721     - f1          : 0.5770    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5112     - val_accuracy: 0.7487     - val_f1: 0.5770    \n",
            "- Step \u001b[96m  625/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4794     - accuracy    : 0.7720     - f1          : 0.5772    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5228     - val_accuracy: 0.7531     - val_f1: 0.5409    \n",
            "- Step \u001b[96m  650/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4764     - accuracy    : 0.7727     - f1          : 0.5785    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5213     - val_accuracy: 0.7478     - val_f1: 0.6124    \n",
            "- Step \u001b[96m  675/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4765     - accuracy    : 0.7722     - f1          : 0.5850    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5541     - val_accuracy: 0.7163     - val_f1: 0.6229    \n",
            "- Step \u001b[96m  700/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4742     - accuracy    : 0.7734     - f1          : 0.5921    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5330     - val_accuracy: 0.7460     - val_f1: 0.6222    \n",
            "- Step \u001b[96m  725/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4736     - accuracy    : 0.7740     - f1          : 0.5930    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5513     - val_accuracy: 0.7433     - val_f1: 0.5311    \n",
            "- Step \u001b[96m  750/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4719     - accuracy    : 0.7740     - f1          : 0.5930    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5262     - val_accuracy: 0.7451     - val_f1: 0.5521    \n",
            "- Step \u001b[96m  775/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4725     - accuracy    : 0.7742     - f1          : 0.5937    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5214     - val_accuracy: 0.7406     - val_f1: 0.5175    \n",
            "- Step \u001b[96m  800/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4734     - accuracy    : 0.7739     - f1          : 0.5955    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5219     - val_accuracy: 0.7352     - val_f1: 0.6194    \n",
            "- Step \u001b[96m  825/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4729     - accuracy    : 0.7741     - f1          : 0.5982    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5012     - val_accuracy: 0.7406     - val_f1: 0.5744    \n",
            "- Step \u001b[96m  850/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4723     - accuracy    : 0.7744     - f1          : 0.5991    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5054     - val_accuracy: 0.7406     - val_f1: 0.5901    \n",
            "- Step \u001b[96m  875/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4700     - accuracy    : 0.7761     - f1          : 0.6026    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5264     - val_accuracy: 0.7540     - val_f1: 0.5692    \n",
            "- Step \u001b[96m  900/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4712     - accuracy    : 0.7760     - f1          : 0.6018    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5181     - val_accuracy: 0.7567     - val_f1: 0.6220    \n",
            "- Step \u001b[96m  925/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4704     - accuracy    : 0.7761     - f1          : 0.6027    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5488     - val_accuracy: 0.7253     - val_f1: 0.6213    \n",
            "- Step \u001b[96m  950/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4692     - accuracy    : 0.7772     - f1          : 0.6051    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5276     - val_accuracy: 0.7531     - val_f1: 0.5409    \n",
            "- Step \u001b[96m  975/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4713     - accuracy    : 0.7763     - f1          : 0.6021    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5124     - val_accuracy: 0.7513     - val_f1: 0.5265    \n",
            "- Step \u001b[96m 1000/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4711     - accuracy    : 0.7762     - f1          : 0.6019    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5255     - val_accuracy: 0.7298     - val_f1: 0.6013    \n",
            "- Step \u001b[96m 1025/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4711     - accuracy    : 0.7767     - f1          : 0.6039    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5104     - val_accuracy: 0.7469     - val_f1: 0.5495    \n",
            "- Step \u001b[96m 1050/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4721     - accuracy    : 0.7764     - f1          : 0.6040    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5120     - val_accuracy: 0.7370     - val_f1: 0.5747    \n",
            "- Step \u001b[96m 1075/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4717     - accuracy    : 0.7771     - f1          : 0.6071    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5191     - val_accuracy: 0.7352     - val_f1: 0.5839    \n",
            "- Step \u001b[96m 1100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4718     - accuracy    : 0.7774     - f1          : 0.6108    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5392     - val_accuracy: 0.7343     - val_f1: 0.6032    \n",
            "- Step \u001b[96m 1125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4711     - accuracy    : 0.7776     - f1          : 0.6105    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5590     - val_accuracy: 0.7244     - val_f1: 0.4130    \n",
            "- Step \u001b[96m 1150/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4697     - accuracy    : 0.7786     - f1          : 0.6097    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5906     - val_accuracy: 0.7226     - val_f1: 0.3783    \n",
            "- Step \u001b[96m 1175/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4720     - accuracy    : 0.7770     - f1          : 0.6076    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5359     - val_accuracy: 0.7334     - val_f1: 0.6158    \n",
            "- Step \u001b[96m 1200/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4720     - accuracy    : 0.7771     - f1          : 0.6125    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5796     - val_accuracy: 0.7065     - val_f1: 0.6338    \n",
            "- Step \u001b[96m 1225/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4719     - accuracy    : 0.7763     - f1          : 0.6130    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4970     - val_accuracy: 0.7558     - val_f1: 0.5737    \n",
            "- Step \u001b[96m 1250/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4710     - accuracy    : 0.7766     - f1          : 0.6119    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4954     - val_accuracy: 0.7612     - val_f1: 0.5831    \n",
            "- Step \u001b[96m 1275/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4692     - accuracy    : 0.7782     - f1          : 0.6148    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5141     - val_accuracy: 0.7540     - val_f1: 0.6347    \n",
            "- Step \u001b[96m 1300/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4686     - accuracy    : 0.7785     - f1          : 0.6150    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5371     - val_accuracy: 0.7639     - val_f1: 0.5871    \n",
            "- Step \u001b[96m 1325/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4683     - accuracy    : 0.7787     - f1          : 0.6148    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5166     - val_accuracy: 0.7720     - val_f1: 0.6402    \n",
            "- Step \u001b[96m 1350/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4683     - accuracy    : 0.7786     - f1          : 0.6145    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5030     - val_accuracy: 0.7729     - val_f1: 0.6252    \n",
            "- Step \u001b[96m 1375/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4675     - accuracy    : 0.7790     - f1          : 0.6165    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4988     - val_accuracy: 0.7675     - val_f1: 0.5997    \n",
            "- Step \u001b[96m 1400/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4672     - accuracy    : 0.7793     - f1          : 0.6170    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4926     - val_accuracy: 0.7747     - val_f1: 0.6368    \n",
            "- Step \u001b[96m 1425/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4673     - accuracy    : 0.7794     - f1          : 0.6168    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4915     - val_accuracy: 0.7747     - val_f1: 0.6378    \n",
            "- Step \u001b[96m 1450/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4659     - accuracy    : 0.7803     - f1          : 0.6177    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5091     - val_accuracy: 0.7594     - val_f1: 0.5548    \n",
            "- Step \u001b[96m 1475/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4661     - accuracy    : 0.7800     - f1          : 0.6171    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5018     - val_accuracy: 0.7594     - val_f1: 0.6116    \n",
            "- Step \u001b[96m 1500/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4661     - accuracy    : 0.7800     - f1          : 0.6169    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5025     - val_accuracy: 0.7594     - val_f1: 0.5964    \n",
            "- Step \u001b[96m 1525/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4671     - accuracy    : 0.7792     - f1          : 0.6155    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4994     - val_accuracy: 0.7630     - val_f1: 0.5769    \n",
            "- Step \u001b[96m 1550/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4679     - accuracy    : 0.7788     - f1          : 0.6145    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5077     - val_accuracy: 0.7496     - val_f1: 0.6265    \n",
            "- Step \u001b[96m 1575/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4671     - accuracy    : 0.7795     - f1          : 0.6163    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4941     - val_accuracy: 0.7603     - val_f1: 0.5700    \n",
            "- Step \u001b[96m 1600/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4665     - accuracy    : 0.7799     - f1          : 0.6160    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5176     - val_accuracy: 0.7558     - val_f1: 0.5211    \n",
            "- Step \u001b[96m 1625/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4656     - accuracy    : 0.7805     - f1          : 0.6157    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5138     - val_accuracy: 0.7666     - val_f1: 0.6579    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m 1650/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4645     - accuracy    : 0.7815     - f1          : 0.6182    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5156     - val_accuracy: 0.7594     - val_f1: 0.6483    \n",
            "- Step \u001b[96m 1675/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4649     - accuracy    : 0.7809     - f1          : 0.6175    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4959     - val_accuracy: 0.7684     - val_f1: 0.6183    \n",
            "- Step \u001b[96m 1700/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4647     - accuracy    : 0.7813     - f1          : 0.6185    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4985     - val_accuracy: 0.7612     - val_f1: 0.6509    \n",
            "- Step \u001b[96m 1725/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4640     - accuracy    : 0.7817     - f1          : 0.6208    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4944     - val_accuracy: 0.7693     - val_f1: 0.6532    \n",
            "- Step \u001b[96m 1750/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4626     - accuracy    : 0.7823     - f1          : 0.6219    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5075     - val_accuracy: 0.7774     - val_f1: 0.6185    \n",
            "- Step \u001b[96m 1775/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4614     - accuracy    : 0.7827     - f1          : 0.6223    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5096     - val_accuracy: 0.7774     - val_f1: 0.6385    \n",
            "- Step \u001b[96m 1800/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4616     - accuracy    : 0.7827     - f1          : 0.6231    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5356     - val_accuracy: 0.7478     - val_f1: 0.6527    \n",
            "- Step \u001b[96m 1825/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4610     - accuracy    : 0.7830     - f1          : 0.6244    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5181     - val_accuracy: 0.7603     - val_f1: 0.6501    \n",
            "- Step \u001b[96m 1850/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4596     - accuracy    : 0.7841     - f1          : 0.6261    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5227     - val_accuracy: 0.7738     - val_f1: 0.5949    \n",
            "- Step \u001b[96m 1875/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4596     - accuracy    : 0.7839     - f1          : 0.6258    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5239     - val_accuracy: 0.7630     - val_f1: 0.6218    \n",
            "- Step \u001b[96m 1900/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4593     - accuracy    : 0.7841     - f1          : 0.6265    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5343     - val_accuracy: 0.7513     - val_f1: 0.6507    \n",
            "- Step \u001b[96m 1925/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4591     - accuracy    : 0.7840     - f1          : 0.6270    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5020     - val_accuracy: 0.7729     - val_f1: 0.6016    \n",
            "- Step \u001b[96m 1950/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4584     - accuracy    : 0.7844     - f1          : 0.6271    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5147     - val_accuracy: 0.7603     - val_f1: 0.5291    \n",
            "- Step \u001b[96m 1975/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4576     - accuracy    : 0.7847     - f1          : 0.6267    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4964     - val_accuracy: 0.7639     - val_f1: 0.6172    \n",
            "- Step \u001b[96m 2000/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4575     - accuracy    : 0.7850     - f1          : 0.6279    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5235     - val_accuracy: 0.7406     - val_f1: 0.6588    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m 2025/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4574     - accuracy    : 0.7850     - f1          : 0.6286    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4919     - val_accuracy: 0.7648     - val_f1: 0.6136    \n",
            "- Step \u001b[96m 2050/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4572     - accuracy    : 0.7851     - f1          : 0.6294    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4968     - val_accuracy: 0.7657     - val_f1: 0.5941    \n",
            "- Step \u001b[96m 2075/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4568     - accuracy    : 0.7854     - f1          : 0.6299    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4945     - val_accuracy: 0.7576     - val_f1: 0.6457    \n",
            "- Step \u001b[96m 2100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4562     - accuracy    : 0.7857     - f1          : 0.6304    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5013     - val_accuracy: 0.7666     - val_f1: 0.5820    \n",
            "- Step \u001b[96m 2125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4547     - accuracy    : 0.7865     - f1          : 0.6314    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5008     - val_accuracy: 0.7855     - val_f1: 0.6470    \n",
            "Epoch \u001b[92m  4/20\u001b[00m:\n",
            "- Step \u001b[96m   25/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4021     - accuracy    : 0.7900     - f1          : 0.6613    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5536     - val_accuracy: 0.7496     - val_f1: 0.6593    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m   50/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3459     - accuracy    : 0.8425     - f1          : 0.7429    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5541     - val_accuracy: 0.7756     - val_f1: 0.6377    \n",
            "- Step \u001b[96m   75/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3829     - accuracy    : 0.8283     - f1          : 0.7253    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5516     - val_accuracy: 0.7792     - val_f1: 0.6227    \n",
            "- Step \u001b[96m  100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4032     - accuracy    : 0.8188     - f1          : 0.7173    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6005     - val_accuracy: 0.7163     - val_f1: 0.6441    \n",
            "- Step \u001b[96m  125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4046     - accuracy    : 0.8220     - f1          : 0.7245    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5022     - val_accuracy: 0.7576     - val_f1: 0.6400    \n",
            "- Step \u001b[96m  150/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4015     - accuracy    : 0.8200     - f1          : 0.7158    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5002     - val_accuracy: 0.7612     - val_f1: 0.5696    \n",
            "- Step \u001b[96m  175/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3981     - accuracy    : 0.8257     - f1          : 0.7109    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.4991     - val_accuracy: 0.7684     - val_f1: 0.6195    \n",
            "- Step \u001b[96m  200/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3973     - accuracy    : 0.8256     - f1          : 0.7048    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5071     - val_accuracy: 0.7567     - val_f1: 0.6123    \n",
            "- Step \u001b[96m  225/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3954     - accuracy    : 0.8272     - f1          : 0.7058    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5357     - val_accuracy: 0.7388     - val_f1: 0.6330    \n",
            "- Step \u001b[96m  250/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3992     - accuracy    : 0.8245     - f1          : 0.7077    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5220     - val_accuracy: 0.7513     - val_f1: 0.6398    \n",
            "- Step \u001b[96m  275/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3952     - accuracy    : 0.8273     - f1          : 0.7081    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5082     - val_accuracy: 0.7594     - val_f1: 0.5635    \n",
            "- Step \u001b[96m  300/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3976     - accuracy    : 0.8254     - f1          : 0.7047    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5152     - val_accuracy: 0.7522     - val_f1: 0.6000    \n",
            "- Step \u001b[96m  325/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4003     - accuracy    : 0.8219     - f1          : 0.6976    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5526     - val_accuracy: 0.7442     - val_f1: 0.6341    \n",
            "- Step \u001b[96m  350/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4005     - accuracy    : 0.8211     - f1          : 0.6991    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5345     - val_accuracy: 0.7424     - val_f1: 0.6019    \n",
            "- Step \u001b[96m  375/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4043     - accuracy    : 0.8210     - f1          : 0.6978    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5303     - val_accuracy: 0.7496     - val_f1: 0.5701    \n",
            "- Step \u001b[96m  400/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.4007     - accuracy    : 0.8219     - f1          : 0.6981    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5272     - val_accuracy: 0.7576     - val_f1: 0.5820    \n",
            "- Step \u001b[96m  425/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3986     - accuracy    : 0.8241     - f1          : 0.6983    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5325     - val_accuracy: 0.7675     - val_f1: 0.6140    \n",
            "- Step \u001b[96m  450/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3984     - accuracy    : 0.8236     - f1          : 0.6952    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5353     - val_accuracy: 0.7675     - val_f1: 0.6614    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m  475/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3955     - accuracy    : 0.8255     - f1          : 0.6999    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5641     - val_accuracy: 0.7469     - val_f1: 0.6536    \n",
            "- Step \u001b[96m  500/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3967     - accuracy    : 0.8238     - f1          : 0.7006    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5463     - val_accuracy: 0.7756     - val_f1: 0.6547    \n",
            "- Step \u001b[96m  525/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3921     - accuracy    : 0.8262     - f1          : 0.7033    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5637     - val_accuracy: 0.7675     - val_f1: 0.6398    \n",
            "- Step \u001b[96m  550/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3898     - accuracy    : 0.8264     - f1          : 0.7027    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5702     - val_accuracy: 0.7648     - val_f1: 0.6431    \n",
            "- Step \u001b[96m  575/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3888     - accuracy    : 0.8263     - f1          : 0.7046    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5598     - val_accuracy: 0.7585     - val_f1: 0.6474    \n",
            "- Step \u001b[96m  600/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3863     - accuracy    : 0.8279     - f1          : 0.7069    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5550     - val_accuracy: 0.7612     - val_f1: 0.6275    \n",
            "- Step \u001b[96m  625/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3875     - accuracy    : 0.8272     - f1          : 0.7063    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5337     - val_accuracy: 0.7693     - val_f1: 0.6158    \n",
            "- Step \u001b[96m  650/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3848     - accuracy    : 0.8283     - f1          : 0.7083    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5210     - val_accuracy: 0.7747     - val_f1: 0.6440    \n",
            "- Step \u001b[96m  675/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3843     - accuracy    : 0.8289     - f1          : 0.7132    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5476     - val_accuracy: 0.7576     - val_f1: 0.6538    \n",
            "- Step \u001b[96m  700/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3816     - accuracy    : 0.8304     - f1          : 0.7178    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5715     - val_accuracy: 0.7675     - val_f1: 0.6447    \n",
            "- Step \u001b[96m  725/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3801     - accuracy    : 0.8317     - f1          : 0.7199    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6022     - val_accuracy: 0.7612     - val_f1: 0.6222    \n",
            "- Step \u001b[96m  750/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3788     - accuracy    : 0.8327     - f1          : 0.7216    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6189     - val_accuracy: 0.7576     - val_f1: 0.5687    \n",
            "- Step \u001b[96m  775/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3810     - accuracy    : 0.8321     - f1          : 0.7195    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5841     - val_accuracy: 0.7594     - val_f1: 0.5442    \n",
            "- Step \u001b[96m  800/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3823     - accuracy    : 0.8312     - f1          : 0.7186    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5671     - val_accuracy: 0.7442     - val_f1: 0.6468    \n",
            "- Step \u001b[96m  825/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3826     - accuracy    : 0.8318     - f1          : 0.7215    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5308     - val_accuracy: 0.7469     - val_f1: 0.6240    \n",
            "- Step \u001b[96m  850/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3821     - accuracy    : 0.8319     - f1          : 0.7213    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5229     - val_accuracy: 0.7738     - val_f1: 0.6147    \n",
            "- Step \u001b[96m  875/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3813     - accuracy    : 0.8323     - f1          : 0.7215    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5404     - val_accuracy: 0.7684     - val_f1: 0.6161    \n",
            "- Step \u001b[96m  900/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3825     - accuracy    : 0.8317     - f1          : 0.7210    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5619     - val_accuracy: 0.7513     - val_f1: 0.6292    \n",
            "- Step \u001b[96m  925/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3805     - accuracy    : 0.8323     - f1          : 0.7212    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5429     - val_accuracy: 0.7612     - val_f1: 0.6356    \n",
            "- Step \u001b[96m  950/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3789     - accuracy    : 0.8334     - f1          : 0.7230    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5511     - val_accuracy: 0.7558     - val_f1: 0.6233    \n",
            "- Step \u001b[96m  975/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3806     - accuracy    : 0.8327     - f1          : 0.7219    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5523     - val_accuracy: 0.7558     - val_f1: 0.5710    \n",
            "- Step \u001b[96m 1000/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3808     - accuracy    : 0.8322     - f1          : 0.7209    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5320     - val_accuracy: 0.7558     - val_f1: 0.5696    \n",
            "- Step \u001b[96m 1025/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3804     - accuracy    : 0.8327     - f1          : 0.7216    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5384     - val_accuracy: 0.7379     - val_f1: 0.6148    \n",
            "- Step \u001b[96m 1050/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3807     - accuracy    : 0.8329     - f1          : 0.7233    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5747     - val_accuracy: 0.7253     - val_f1: 0.6250    \n",
            "- Step \u001b[96m 1075/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3800     - accuracy    : 0.8328     - f1          : 0.7246    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5538     - val_accuracy: 0.7424     - val_f1: 0.5835    \n",
            "- Step \u001b[96m 1100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3807     - accuracy    : 0.8327     - f1          : 0.7255    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5983     - val_accuracy: 0.7415     - val_f1: 0.6260    \n",
            "- Step \u001b[96m 1125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3814     - accuracy    : 0.8321     - f1          : 0.7246    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5569     - val_accuracy: 0.7415     - val_f1: 0.5752    \n",
            "- Step \u001b[96m 1150/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3813     - accuracy    : 0.8322     - f1          : 0.7233    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6214     - val_accuracy: 0.7379     - val_f1: 0.4065    \n",
            "- Step \u001b[96m 1175/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3845     - accuracy    : 0.8294     - f1          : 0.7182    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5124     - val_accuracy: 0.7496     - val_f1: 0.5740    \n",
            "- Step \u001b[96m 1200/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3853     - accuracy    : 0.8295     - f1          : 0.7208    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6130     - val_accuracy: 0.6912     - val_f1: 0.6301    \n",
            "- Step \u001b[96m 1225/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3862     - accuracy    : 0.8289     - f1          : 0.7218    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5206     - val_accuracy: 0.7531     - val_f1: 0.6405    \n",
            "- Step \u001b[96m 1250/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3861     - accuracy    : 0.8286     - f1          : 0.7207    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5969     - val_accuracy: 0.7549     - val_f1: 0.5081    \n",
            "- Step \u001b[96m 1275/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3856     - accuracy    : 0.8288     - f1          : 0.7198    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5307     - val_accuracy: 0.7639     - val_f1: 0.6248    \n",
            "- Step \u001b[96m 1300/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3849     - accuracy    : 0.8293     - f1          : 0.7205    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5571     - val_accuracy: 0.7504     - val_f1: 0.6490    \n",
            "- Step \u001b[96m 1325/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3841     - accuracy    : 0.8298     - f1          : 0.7215    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5570     - val_accuracy: 0.7666     - val_f1: 0.6286    \n",
            "- Step \u001b[96m 1350/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3836     - accuracy    : 0.8303     - f1          : 0.7217    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5587     - val_accuracy: 0.7558     - val_f1: 0.6035    \n",
            "- Step \u001b[96m 1375/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3825     - accuracy    : 0.8312     - f1          : 0.7238    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5703     - val_accuracy: 0.7415     - val_f1: 0.6327    \n",
            "- Step \u001b[96m 1400/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3826     - accuracy    : 0.8311     - f1          : 0.7242    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5627     - val_accuracy: 0.7442     - val_f1: 0.6313    \n",
            "- Step \u001b[96m 1425/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3830     - accuracy    : 0.8306     - f1          : 0.7231    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5489     - val_accuracy: 0.7549     - val_f1: 0.6015    \n",
            "- Step \u001b[96m 1450/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3821     - accuracy    : 0.8314     - f1          : 0.7240    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5991     - val_accuracy: 0.7567     - val_f1: 0.5650    \n",
            "- Step \u001b[96m 1475/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3829     - accuracy    : 0.8312     - f1          : 0.7235    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5535     - val_accuracy: 0.7540     - val_f1: 0.5898    \n",
            "- Step \u001b[96m 1500/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3828     - accuracy    : 0.8313     - f1          : 0.7229    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5435     - val_accuracy: 0.7513     - val_f1: 0.5980    \n",
            "- Step \u001b[96m 1525/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3836     - accuracy    : 0.8312     - f1          : 0.7231    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5339     - val_accuracy: 0.7504     - val_f1: 0.6160    \n",
            "- Step \u001b[96m 1550/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3846     - accuracy    : 0.8306     - f1          : 0.7224    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5298     - val_accuracy: 0.7415     - val_f1: 0.6364    \n",
            "- Step \u001b[96m 1575/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3837     - accuracy    : 0.8310     - f1          : 0.7236    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5243     - val_accuracy: 0.7540     - val_f1: 0.6119    \n",
            "- Step \u001b[96m 1600/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3827     - accuracy    : 0.8320     - f1          : 0.7246    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5699     - val_accuracy: 0.7612     - val_f1: 0.5831    \n",
            "- Step \u001b[96m 1625/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3823     - accuracy    : 0.8322     - f1          : 0.7240    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5503     - val_accuracy: 0.7693     - val_f1: 0.6281    \n",
            "- Step \u001b[96m 1650/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3817     - accuracy    : 0.8328     - f1          : 0.7251    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5716     - val_accuracy: 0.7397     - val_f1: 0.6572    \n",
            "- Step \u001b[96m 1675/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3821     - accuracy    : 0.8325     - f1          : 0.7257    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5340     - val_accuracy: 0.7540     - val_f1: 0.6442    \n",
            "- Step \u001b[96m 1700/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3817     - accuracy    : 0.8324     - f1          : 0.7257    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5128     - val_accuracy: 0.7675     - val_f1: 0.6273    \n",
            "- Step \u001b[96m 1725/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3815     - accuracy    : 0.8322     - f1          : 0.7260    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5153     - val_accuracy: 0.7576     - val_f1: 0.6466    \n",
            "- Step \u001b[96m 1750/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3799     - accuracy    : 0.8328     - f1          : 0.7269    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5322     - val_accuracy: 0.7657     - val_f1: 0.6255    \n",
            "- Step \u001b[96m 1775/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3786     - accuracy    : 0.8333     - f1          : 0.7274    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5623     - val_accuracy: 0.7657     - val_f1: 0.6234    \n",
            "- Step \u001b[96m 1800/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3792     - accuracy    : 0.8331     - f1          : 0.7273    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5667     - val_accuracy: 0.7594     - val_f1: 0.6675    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m 1825/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3785     - accuracy    : 0.8332     - f1          : 0.7279    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5647     - val_accuracy: 0.7424     - val_f1: 0.6538    \n",
            "- Step \u001b[96m 1850/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3763     - accuracy    : 0.8345     - f1          : 0.7300    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5802     - val_accuracy: 0.7603     - val_f1: 0.5769    \n",
            "- Step \u001b[96m 1875/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3768     - accuracy    : 0.8343     - f1          : 0.7298    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5549     - val_accuracy: 0.7666     - val_f1: 0.6000    \n",
            "- Step \u001b[96m 1900/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3768     - accuracy    : 0.8346     - f1          : 0.7301    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5351     - val_accuracy: 0.7442     - val_f1: 0.6486    \n",
            "- Step \u001b[96m 1925/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3766     - accuracy    : 0.8350     - f1          : 0.7312    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5039     - val_accuracy: 0.7693     - val_f1: 0.6503    \n",
            "- Step \u001b[96m 1950/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3762     - accuracy    : 0.8349     - f1          : 0.7307    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5590     - val_accuracy: 0.7603     - val_f1: 0.4991    \n",
            "- Step \u001b[96m 1975/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3763     - accuracy    : 0.8344     - f1          : 0.7288    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5311     - val_accuracy: 0.7711     - val_f1: 0.5626    \n",
            "- Step \u001b[96m 2000/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3762     - accuracy    : 0.8343     - f1          : 0.7286    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5256     - val_accuracy: 0.7567     - val_f1: 0.6658    \n",
            "- Step \u001b[96m 2025/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3769     - accuracy    : 0.8342     - f1          : 0.7287    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5244     - val_accuracy: 0.7487     - val_f1: 0.6667    \n",
            "- Step \u001b[96m 2050/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3766     - accuracy    : 0.8345     - f1          : 0.7303    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5127     - val_accuracy: 0.7738     - val_f1: 0.6193    \n",
            "- Step \u001b[96m 2075/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3765     - accuracy    : 0.8348     - f1          : 0.7304    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5154     - val_accuracy: 0.7774     - val_f1: 0.6220    \n",
            "- Step \u001b[96m 2100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3759     - accuracy    : 0.8349     - f1          : 0.7305    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5118     - val_accuracy: 0.7756     - val_f1: 0.6693    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m 2125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3742     - accuracy    : 0.8358     - f1          : 0.7320    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5335     - val_accuracy: 0.7783     - val_f1: 0.6612    \n",
            "Epoch \u001b[92m  5/20\u001b[00m:\n",
            "- Step \u001b[96m   25/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3038     - accuracy    : 0.8650     - f1          : 0.7652    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5851     - val_accuracy: 0.7819     - val_f1: 0.6756    \n",
            "There is already a model saved with the name xlm-roberta-large-qa, which will be overwritten by new version!\n",
            "- Step \u001b[96m   50/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2745     - accuracy    : 0.8800     - f1          : 0.8000    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6308     - val_accuracy: 0.7603     - val_f1: 0.6599    \n",
            "- Step \u001b[96m   75/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3049     - accuracy    : 0.8683     - f1          : 0.7916    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5866     - val_accuracy: 0.7783     - val_f1: 0.6206    \n",
            "- Step \u001b[96m  100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3231     - accuracy    : 0.8588     - f1          : 0.7780    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5720     - val_accuracy: 0.7567     - val_f1: 0.6642    \n",
            "- Step \u001b[96m  125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3230     - accuracy    : 0.8590     - f1          : 0.7807    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5312     - val_accuracy: 0.7657     - val_f1: 0.6717    \n",
            "- Step \u001b[96m  150/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3175     - accuracy    : 0.8617     - f1          : 0.7839    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5084     - val_accuracy: 0.7801     - val_f1: 0.6316    \n",
            "- Step \u001b[96m  175/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3128     - accuracy    : 0.8679     - f1          : 0.7831    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5361     - val_accuracy: 0.7792     - val_f1: 0.6070    \n",
            "- Step \u001b[96m  200/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3157     - accuracy    : 0.8625     - f1          : 0.7699    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5328     - val_accuracy: 0.7648     - val_f1: 0.6225    \n",
            "- Step \u001b[96m  225/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3126     - accuracy    : 0.8633     - f1          : 0.7709    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6249     - val_accuracy: 0.7352     - val_f1: 0.6613    \n",
            "- Step \u001b[96m  250/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3154     - accuracy    : 0.8615     - f1          : 0.7724    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5545     - val_accuracy: 0.7594     - val_f1: 0.6492    \n",
            "- Step \u001b[96m  275/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3116     - accuracy    : 0.8664     - f1          : 0.7769    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5613     - val_accuracy: 0.7684     - val_f1: 0.6172    \n",
            "- Step \u001b[96m  300/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3109     - accuracy    : 0.8675     - f1          : 0.7804    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5831     - val_accuracy: 0.7621     - val_f1: 0.6109    \n",
            "- Step \u001b[96m  325/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3167     - accuracy    : 0.8650     - f1          : 0.7737    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6070     - val_accuracy: 0.7496     - val_f1: 0.6225    \n",
            "- Step \u001b[96m  350/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3184     - accuracy    : 0.8646     - f1          : 0.7751    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6274     - val_accuracy: 0.7244     - val_f1: 0.6089    \n",
            "- Step \u001b[96m  375/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3208     - accuracy    : 0.8640     - f1          : 0.7736    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6091     - val_accuracy: 0.7406     - val_f1: 0.6025    \n",
            "- Step \u001b[96m  400/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3166     - accuracy    : 0.8669     - f1          : 0.7777    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6276     - val_accuracy: 0.7594     - val_f1: 0.5851    \n",
            "- Step \u001b[96m  425/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3134     - accuracy    : 0.8674     - f1          : 0.7751    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6343     - val_accuracy: 0.7567     - val_f1: 0.6167    \n",
            "- Step \u001b[96m  450/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3156     - accuracy    : 0.8656     - f1          : 0.7702    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6324     - val_accuracy: 0.7154     - val_f1: 0.6213    \n",
            "- Step \u001b[96m  475/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3121     - accuracy    : 0.8671     - f1          : 0.7753    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6190     - val_accuracy: 0.7558     - val_f1: 0.6402    \n",
            "- Step \u001b[96m  500/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3133     - accuracy    : 0.8670     - f1          : 0.7765    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6168     - val_accuracy: 0.7558     - val_f1: 0.6274    \n",
            "- Step \u001b[96m  525/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3099     - accuracy    : 0.8686     - f1          : 0.7787    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6571     - val_accuracy: 0.7442     - val_f1: 0.6397    \n",
            "- Step \u001b[96m  550/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3080     - accuracy    : 0.8693     - f1          : 0.7798    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6616     - val_accuracy: 0.7684     - val_f1: 0.6272    \n",
            "- Step \u001b[96m  575/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3060     - accuracy    : 0.8704     - f1          : 0.7825    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6787     - val_accuracy: 0.7666     - val_f1: 0.6254    \n",
            "- Step \u001b[96m  600/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3034     - accuracy    : 0.8719     - f1          : 0.7844    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6777     - val_accuracy: 0.7549     - val_f1: 0.6486    \n",
            "- Step \u001b[96m  625/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3024     - accuracy    : 0.8722     - f1          : 0.7862    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6736     - val_accuracy: 0.7621     - val_f1: 0.6452    \n",
            "- Step \u001b[96m  650/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3001     - accuracy    : 0.8731     - f1          : 0.7882    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6463     - val_accuracy: 0.7513     - val_f1: 0.6370    \n",
            "- Step \u001b[96m  675/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2989     - accuracy    : 0.8733     - f1          : 0.7911    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6361     - val_accuracy: 0.7217     - val_f1: 0.6283    \n",
            "- Step \u001b[96m  700/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2953     - accuracy    : 0.8755     - f1          : 0.7961    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6778     - val_accuracy: 0.7513     - val_f1: 0.6272    \n",
            "- Step \u001b[96m  725/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2948     - accuracy    : 0.8766     - f1          : 0.7979    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7516     - val_accuracy: 0.7648     - val_f1: 0.6124    \n",
            "- Step \u001b[96m  750/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2957     - accuracy    : 0.8762     - f1          : 0.7967    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7061     - val_accuracy: 0.7558     - val_f1: 0.6035    \n",
            "- Step \u001b[96m  775/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2962     - accuracy    : 0.8768     - f1          : 0.7979    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6593     - val_accuracy: 0.7496     - val_f1: 0.5740    \n",
            "- Step \u001b[96m  800/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2983     - accuracy    : 0.8761     - f1          : 0.7974    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6056     - val_accuracy: 0.7504     - val_f1: 0.6233    \n",
            "- Step \u001b[96m  825/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2979     - accuracy    : 0.8771     - f1          : 0.7998    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6021     - val_accuracy: 0.7424     - val_f1: 0.6168    \n",
            "- Step \u001b[96m  850/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2970     - accuracy    : 0.8784     - f1          : 0.8019    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6255     - val_accuracy: 0.7603     - val_f1: 0.6136    \n",
            "- Step \u001b[96m  875/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2962     - accuracy    : 0.8789     - f1          : 0.8022    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6537     - val_accuracy: 0.7540     - val_f1: 0.6086    \n",
            "- Step \u001b[96m  900/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2978     - accuracy    : 0.8779     - f1          : 0.8011    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6574     - val_accuracy: 0.7451     - val_f1: 0.6223    \n",
            "- Step \u001b[96m  925/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2946     - accuracy    : 0.8796     - f1          : 0.8031    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6697     - val_accuracy: 0.7415     - val_f1: 0.6201    \n",
            "- Step \u001b[96m  950/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2931     - accuracy    : 0.8807     - f1          : 0.8047    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7099     - val_accuracy: 0.7424     - val_f1: 0.6372    \n",
            "- Step \u001b[96m  975/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2956     - accuracy    : 0.8792     - f1          : 0.8031    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6819     - val_accuracy: 0.7576     - val_f1: 0.5994    \n",
            "- Step \u001b[96m 1000/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2958     - accuracy    : 0.8792     - f1          : 0.8030    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6644     - val_accuracy: 0.7585     - val_f1: 0.5583    \n",
            "- Step \u001b[96m 1025/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2957     - accuracy    : 0.8796     - f1          : 0.8032    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6003     - val_accuracy: 0.7433     - val_f1: 0.6207    \n",
            "- Step \u001b[96m 1050/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2967     - accuracy    : 0.8790     - f1          : 0.8034    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6236     - val_accuracy: 0.7244     - val_f1: 0.6315    \n",
            "- Step \u001b[96m 1075/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2965     - accuracy    : 0.8793     - f1          : 0.8048    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5883     - val_accuracy: 0.7585     - val_f1: 0.5991    \n",
            "- Step \u001b[96m 1100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2974     - accuracy    : 0.8791     - f1          : 0.8051    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6232     - val_accuracy: 0.7343     - val_f1: 0.5866    \n",
            "- Step \u001b[96m 1125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2968     - accuracy    : 0.8790     - f1          : 0.8045    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7029     - val_accuracy: 0.6984     - val_f1: 0.5962    \n",
            "- Step \u001b[96m 1150/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2980     - accuracy    : 0.8783     - f1          : 0.8026    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6170     - val_accuracy: 0.7531     - val_f1: 0.5814    \n",
            "- Step \u001b[96m 1175/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2999     - accuracy    : 0.8765     - f1          : 0.8001    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6078     - val_accuracy: 0.7576     - val_f1: 0.6006    \n",
            "- Step \u001b[96m 1200/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3007     - accuracy    : 0.8759     - f1          : 0.8003    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6468     - val_accuracy: 0.7397     - val_f1: 0.6455    \n",
            "- Step \u001b[96m 1225/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3006     - accuracy    : 0.8756     - f1          : 0.8006    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5947     - val_accuracy: 0.7504     - val_f1: 0.6436    \n",
            "- Step \u001b[96m 1250/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3005     - accuracy    : 0.8758     - f1          : 0.8010    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6296     - val_accuracy: 0.7639     - val_f1: 0.5751    \n",
            "- Step \u001b[96m 1275/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2993     - accuracy    : 0.8764     - f1          : 0.8014    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6156     - val_accuracy: 0.7621     - val_f1: 0.6074    \n",
            "- Step \u001b[96m 1300/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2989     - accuracy    : 0.8766     - f1          : 0.8016    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6348     - val_accuracy: 0.7630     - val_f1: 0.6403    \n",
            "- Step \u001b[96m 1325/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2978     - accuracy    : 0.8771     - f1          : 0.8025    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6486     - val_accuracy: 0.7567     - val_f1: 0.6485    \n",
            "- Step \u001b[96m 1350/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2987     - accuracy    : 0.8767     - f1          : 0.8018    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6338     - val_accuracy: 0.7657     - val_f1: 0.6063    \n",
            "- Step \u001b[96m 1375/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2988     - accuracy    : 0.8766     - f1          : 0.8017    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6149     - val_accuracy: 0.7747     - val_f1: 0.6133    \n",
            "- Step \u001b[96m 1400/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2991     - accuracy    : 0.8766     - f1          : 0.8017    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5549     - val_accuracy: 0.7540     - val_f1: 0.6523    \n",
            "- Step \u001b[96m 1425/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2998     - accuracy    : 0.8764     - f1          : 0.8012    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5468     - val_accuracy: 0.7621     - val_f1: 0.6345    \n",
            "- Step \u001b[96m 1450/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2991     - accuracy    : 0.8770     - f1          : 0.8018    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6169     - val_accuracy: 0.7639     - val_f1: 0.5819    \n",
            "- Step \u001b[96m 1475/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2994     - accuracy    : 0.8770     - f1          : 0.8019    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5827     - val_accuracy: 0.7648     - val_f1: 0.6136    \n",
            "- Step \u001b[96m 1500/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2998     - accuracy    : 0.8767     - f1          : 0.8008    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5726     - val_accuracy: 0.7522     - val_f1: 0.6209    \n",
            "- Step \u001b[96m 1525/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3003     - accuracy    : 0.8764     - f1          : 0.8008    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5885     - val_accuracy: 0.7603     - val_f1: 0.6307    \n",
            "- Step \u001b[96m 1550/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3016     - accuracy    : 0.8756     - f1          : 0.7994    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5828     - val_accuracy: 0.7442     - val_f1: 0.6379    \n",
            "- Step \u001b[96m 1575/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.3005     - accuracy    : 0.8761     - f1          : 0.8006    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5814     - val_accuracy: 0.7576     - val_f1: 0.6503    \n",
            "- Step \u001b[96m 1600/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2998     - accuracy    : 0.8767     - f1          : 0.8014    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6882     - val_accuracy: 0.7675     - val_f1: 0.5947    \n",
            "- Step \u001b[96m 1625/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2993     - accuracy    : 0.8770     - f1          : 0.8013    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6449     - val_accuracy: 0.7612     - val_f1: 0.6111    \n",
            "- Step \u001b[96m 1650/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2985     - accuracy    : 0.8773     - f1          : 0.8018    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6127     - val_accuracy: 0.7540     - val_f1: 0.6584    \n",
            "- Step \u001b[96m 1675/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2996     - accuracy    : 0.8767     - f1          : 0.8015    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5989     - val_accuracy: 0.7504     - val_f1: 0.6525    \n",
            "- Step \u001b[96m 1700/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2985     - accuracy    : 0.8771     - f1          : 0.8025    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5875     - val_accuracy: 0.7693     - val_f1: 0.6259    \n",
            "- Step \u001b[96m 1725/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2983     - accuracy    : 0.8770     - f1          : 0.8026    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6038     - val_accuracy: 0.7630     - val_f1: 0.6562    \n",
            "- Step \u001b[96m 1750/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2973     - accuracy    : 0.8774     - f1          : 0.8031    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6394     - val_accuracy: 0.7702     - val_f1: 0.6614    \n",
            "- Step \u001b[96m 1775/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2962     - accuracy    : 0.8779     - f1          : 0.8041    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6951     - val_accuracy: 0.7693     - val_f1: 0.6181    \n",
            "- Step \u001b[96m 1800/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2973     - accuracy    : 0.8776     - f1          : 0.8037    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6238     - val_accuracy: 0.7540     - val_f1: 0.6566    \n",
            "- Step \u001b[96m 1825/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2967     - accuracy    : 0.8780     - f1          : 0.8046    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6015     - val_accuracy: 0.7513     - val_f1: 0.6550    \n",
            "- Step \u001b[96m 1850/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2950     - accuracy    : 0.8789     - f1          : 0.8061    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6253     - val_accuracy: 0.7648     - val_f1: 0.6181    \n",
            "- Step \u001b[96m 1875/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2953     - accuracy    : 0.8788     - f1          : 0.8061    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6299     - val_accuracy: 0.7729     - val_f1: 0.6296    \n",
            "- Step \u001b[96m 1900/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2950     - accuracy    : 0.8791     - f1          : 0.8064    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5981     - val_accuracy: 0.7657     - val_f1: 0.6497    \n",
            "- Step \u001b[96m 1925/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2944     - accuracy    : 0.8794     - f1          : 0.8071    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6001     - val_accuracy: 0.7558     - val_f1: 0.6566    \n",
            "- Step \u001b[96m 1950/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2945     - accuracy    : 0.8793     - f1          : 0.8069    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6062     - val_accuracy: 0.7738     - val_f1: 0.6205    \n",
            "- Step \u001b[96m 1975/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2941     - accuracy    : 0.8795     - f1          : 0.8068    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6508     - val_accuracy: 0.7648     - val_f1: 0.5719    \n",
            "- Step \u001b[96m 2000/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2947     - accuracy    : 0.8794     - f1          : 0.8064    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5590     - val_accuracy: 0.7603     - val_f1: 0.6445    \n",
            "- Step \u001b[96m 2025/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2951     - accuracy    : 0.8792     - f1          : 0.8063    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5727     - val_accuracy: 0.7316     - val_f1: 0.6543    \n",
            "- Step \u001b[96m 2050/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2957     - accuracy    : 0.8788     - f1          : 0.8064    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5451     - val_accuracy: 0.7693     - val_f1: 0.6396    \n",
            "- Step \u001b[96m 2075/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2956     - accuracy    : 0.8790     - f1          : 0.8066    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6053     - val_accuracy: 0.7738     - val_f1: 0.6038    \n",
            "- Step \u001b[96m 2100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2952     - accuracy    : 0.8791     - f1          : 0.8065    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5866     - val_accuracy: 0.7720     - val_f1: 0.6382    \n",
            "- Step \u001b[96m 2125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2938     - accuracy    : 0.8795     - f1          : 0.8072    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6271     - val_accuracy: 0.7558     - val_f1: 0.6634    \n",
            "Epoch \u001b[92m  6/20\u001b[00m:\n",
            "- Step \u001b[96m   25/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2900     - accuracy    : 0.8650     - f1          : 0.7970    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6416     - val_accuracy: 0.7774     - val_f1: 0.6745    \n",
            "- Step \u001b[96m   50/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2548     - accuracy    : 0.8925     - f1          : 0.8259    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6512     - val_accuracy: 0.7810     - val_f1: 0.6583    \n",
            "- Step \u001b[96m   75/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2648     - accuracy    : 0.8950     - f1          : 0.8389    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6607     - val_accuracy: 0.7774     - val_f1: 0.6507    \n",
            "- Step \u001b[96m  100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2791     - accuracy    : 0.8888     - f1          : 0.8318    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6652     - val_accuracy: 0.7549     - val_f1: 0.6625    \n",
            "- Step \u001b[96m  125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2787     - accuracy    : 0.8860     - f1          : 0.8283    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6423     - val_accuracy: 0.7487     - val_f1: 0.6729    \n",
            "- Step \u001b[96m  150/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2687     - accuracy    : 0.8908     - f1          : 0.8352    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6026     - val_accuracy: 0.7792     - val_f1: 0.6192    \n",
            "- Step \u001b[96m  175/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2643     - accuracy    : 0.8936     - f1          : 0.8305    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5928     - val_accuracy: 0.7801     - val_f1: 0.6105    \n",
            "- Step \u001b[96m  200/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2635     - accuracy    : 0.8956     - f1          : 0.8311    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5680     - val_accuracy: 0.7531     - val_f1: 0.6470    \n",
            "- Step \u001b[96m  225/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2634     - accuracy    : 0.8939     - f1          : 0.8299    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6180     - val_accuracy: 0.7433     - val_f1: 0.6571    \n",
            "- Step \u001b[96m  250/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2619     - accuracy    : 0.8935     - f1          : 0.8305    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6058     - val_accuracy: 0.7567     - val_f1: 0.6401    \n",
            "- Step \u001b[96m  275/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2627     - accuracy    : 0.8945     - f1          : 0.8297    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6186     - val_accuracy: 0.7639     - val_f1: 0.6161    \n",
            "- Step \u001b[96m  300/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2613     - accuracy    : 0.8958     - f1          : 0.8324    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6160     - val_accuracy: 0.7576     - val_f1: 0.6208    \n",
            "- Step \u001b[96m  325/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2663     - accuracy    : 0.8942     - f1          : 0.8280    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6519     - val_accuracy: 0.7504     - val_f1: 0.6243    \n",
            "- Step \u001b[96m  350/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2705     - accuracy    : 0.8914     - f1          : 0.8249    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6787     - val_accuracy: 0.7424     - val_f1: 0.6278    \n",
            "- Step \u001b[96m  375/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2741     - accuracy    : 0.8900     - f1          : 0.8226    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6720     - val_accuracy: 0.7451     - val_f1: 0.6203    \n",
            "- Step \u001b[96m  400/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2690     - accuracy    : 0.8922     - f1          : 0.8255    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7211     - val_accuracy: 0.7460     - val_f1: 0.5626    \n",
            "- Step \u001b[96m  425/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2702     - accuracy    : 0.8918     - f1          : 0.8217    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7162     - val_accuracy: 0.7513     - val_f1: 0.5835    \n",
            "- Step \u001b[96m  450/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2681     - accuracy    : 0.8925     - f1          : 0.8211    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6752     - val_accuracy: 0.7262     - val_f1: 0.6391    \n",
            "- Step \u001b[96m  475/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2656     - accuracy    : 0.8950     - f1          : 0.8269    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6949     - val_accuracy: 0.7244     - val_f1: 0.6375    \n",
            "- Step \u001b[96m  500/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2656     - accuracy    : 0.8942     - f1          : 0.8287    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6985     - val_accuracy: 0.7666     - val_f1: 0.6348    \n",
            "- Step \u001b[96m  525/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2609     - accuracy    : 0.8960     - f1          : 0.8303    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7308     - val_accuracy: 0.7666     - val_f1: 0.6338    \n",
            "- Step \u001b[96m  550/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2585     - accuracy    : 0.8968     - f1          : 0.8312    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7093     - val_accuracy: 0.7576     - val_f1: 0.6466    \n",
            "- Step \u001b[96m  575/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2589     - accuracy    : 0.8965     - f1          : 0.8318    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7059     - val_accuracy: 0.7612     - val_f1: 0.6444    \n",
            "- Step \u001b[96m  600/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2572     - accuracy    : 0.8973     - f1          : 0.8326    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7021     - val_accuracy: 0.7630     - val_f1: 0.6354    \n",
            "- Step \u001b[96m  625/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2571     - accuracy    : 0.8966     - f1          : 0.8318    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6812     - val_accuracy: 0.7594     - val_f1: 0.6267    \n",
            "- Step \u001b[96m  650/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2538     - accuracy    : 0.8979     - f1          : 0.8340    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6630     - val_accuracy: 0.7603     - val_f1: 0.6337    \n",
            "- Step \u001b[96m  675/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2528     - accuracy    : 0.8976     - f1          : 0.8354    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6903     - val_accuracy: 0.7316     - val_f1: 0.6503    \n",
            "- Step \u001b[96m  700/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2500     - accuracy    : 0.8989     - f1          : 0.8387    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7228     - val_accuracy: 0.7478     - val_f1: 0.6336    \n",
            "- Step \u001b[96m  725/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2482     - accuracy    : 0.8998     - f1          : 0.8401    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.8950     - val_accuracy: 0.7603     - val_f1: 0.5659    \n",
            "- Step \u001b[96m  750/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2495     - accuracy    : 0.9002     - f1          : 0.8399    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7739     - val_accuracy: 0.7630     - val_f1: 0.5938    \n",
            "- Step \u001b[96m  775/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2506     - accuracy    : 0.9002     - f1          : 0.8398    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6830     - val_accuracy: 0.7585     - val_f1: 0.6184    \n",
            "- Step \u001b[96m  800/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2521     - accuracy    : 0.8998     - f1          : 0.8400    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6272     - val_accuracy: 0.7496     - val_f1: 0.6162    \n",
            "- Step \u001b[96m  825/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2512     - accuracy    : 0.9008     - f1          : 0.8417    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6554     - val_accuracy: 0.7513     - val_f1: 0.6370    \n",
            "- Step \u001b[96m  850/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2499     - accuracy    : 0.9018     - f1          : 0.8433    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7291     - val_accuracy: 0.7460     - val_f1: 0.6367    \n",
            "- Step \u001b[96m  875/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2492     - accuracy    : 0.9021     - f1          : 0.8440    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7620     - val_accuracy: 0.7612     - val_f1: 0.5895    \n",
            "- Step \u001b[96m  900/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2531     - accuracy    : 0.9003     - f1          : 0.8407    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7214     - val_accuracy: 0.7603     - val_f1: 0.5973    \n",
            "- Step \u001b[96m  925/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2516     - accuracy    : 0.9012     - f1          : 0.8417    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7717     - val_accuracy: 0.7101     - val_f1: 0.6383    \n",
            "- Step \u001b[96m  950/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2524     - accuracy    : 0.9007     - f1          : 0.8412    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6679     - val_accuracy: 0.7415     - val_f1: 0.6400    \n",
            "- Step \u001b[96m  975/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2532     - accuracy    : 0.9001     - f1          : 0.8403    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6797     - val_accuracy: 0.7558     - val_f1: 0.5627    \n",
            "- Step \u001b[96m 1000/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2534     - accuracy    : 0.9006     - f1          : 0.8410    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6917     - val_accuracy: 0.7469     - val_f1: 0.5188    \n",
            "- Step \u001b[96m 1025/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2537     - accuracy    : 0.9002     - f1          : 0.8401    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6309     - val_accuracy: 0.7424     - val_f1: 0.6168    \n",
            "- Step \u001b[96m 1050/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2552     - accuracy    : 0.8996     - f1          : 0.8401    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7297     - val_accuracy: 0.7065     - val_f1: 0.6411    \n",
            "- Step \u001b[96m 1075/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2576     - accuracy    : 0.8983     - f1          : 0.8389    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5933     - val_accuracy: 0.7496     - val_f1: 0.6235    \n",
            "- Step \u001b[96m 1100/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2582     - accuracy    : 0.8981     - f1          : 0.8393    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6163     - val_accuracy: 0.7558     - val_f1: 0.5655    \n",
            "- Step \u001b[96m 1125/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2583     - accuracy    : 0.8981     - f1          : 0.8388    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6032     - val_accuracy: 0.7513     - val_f1: 0.6071    \n",
            "- Step \u001b[96m 1150/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2586     - accuracy    : 0.8978     - f1          : 0.8377    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5993     - val_accuracy: 0.7594     - val_f1: 0.6127    \n",
            "- Step \u001b[96m 1175/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2606     - accuracy    : 0.8963     - f1          : 0.8358    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5943     - val_accuracy: 0.7603     - val_f1: 0.5961    \n",
            "- Step \u001b[96m 1200/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2615     - accuracy    : 0.8955     - f1          : 0.8351    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.5940     - val_accuracy: 0.7424     - val_f1: 0.6344    \n",
            "- Step \u001b[96m 1225/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2617     - accuracy    : 0.8951     - f1          : 0.8348    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6528     - val_accuracy: 0.7190     - val_f1: 0.6365    \n",
            "- Step \u001b[96m 1250/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2613     - accuracy    : 0.8950     - f1          : 0.8348    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6233     - val_accuracy: 0.7451     - val_f1: 0.6120    \n",
            "- Step \u001b[96m 1275/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2596     - accuracy    : 0.8958     - f1          : 0.8359    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7363     - val_accuracy: 0.7549     - val_f1: 0.5561    \n",
            "- Step \u001b[96m 1300/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2599     - accuracy    : 0.8962     - f1          : 0.8357    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7035     - val_accuracy: 0.7487     - val_f1: 0.6196    \n",
            "- Step \u001b[96m 1325/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2597     - accuracy    : 0.8958     - f1          : 0.8356    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7567     - val_accuracy: 0.7226     - val_f1: 0.6428    \n",
            "- Step \u001b[96m 1350/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2608     - accuracy    : 0.8956     - f1          : 0.8351    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6773     - val_accuracy: 0.7576     - val_f1: 0.6229    \n",
            "- Step \u001b[96m 1375/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2610     - accuracy    : 0.8958     - f1          : 0.8356    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7244     - val_accuracy: 0.7594     - val_f1: 0.5677    \n",
            "- Step \u001b[96m 1400/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2606     - accuracy    : 0.8962     - f1          : 0.8360    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6074     - val_accuracy: 0.7531     - val_f1: 0.6228    \n",
            "- Step \u001b[96m 1425/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2610     - accuracy    : 0.8962     - f1          : 0.8359    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6008     - val_accuracy: 0.7370     - val_f1: 0.6342    \n",
            "- Step \u001b[96m 1450/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2603     - accuracy    : 0.8967     - f1          : 0.8367    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6604     - val_accuracy: 0.7612     - val_f1: 0.6077    \n",
            "- Step \u001b[96m 1475/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2611     - accuracy    : 0.8964     - f1          : 0.8359    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.7114     - val_accuracy: 0.7567     - val_f1: 0.5593    \n",
            "- Step \u001b[96m 1500/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2612     - accuracy    : 0.8964     - f1          : 0.8355    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6642     - val_accuracy: 0.7612     - val_f1: 0.5994    \n",
            "- Step \u001b[96m 1525/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2625     - accuracy    : 0.8954     - f1          : 0.8342    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6265     - val_accuracy: 0.7469     - val_f1: 0.6439    \n",
            "- Step \u001b[96m 1550/2125\u001b[00m:\n",
            "    \u001b[95mTrain result        \u001b[00m - loss    : 0.2637     - accuracy    : 0.8945     - f1          : 0.8330    \n",
            "    \u001b[95mValidation result   \u001b[00m - val_loss: 0.6023     - val_accuracy: 0.7433     - val_f1: 0.6521    \n",
            "- Step \u001b[96m 1558/2125\u001b[00m:"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_writer.close()"
      ],
      "metadata": {
        "id": "8dcUGPGYVlaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [N, n, H, Tq, Tv]\n",
        "w = torch.randn((2, 4, 8, 14, 80))"
      ],
      "metadata": {
        "id": "EyoRVUT1vmkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a1 = torch.randn((2, 4, 8, 14, 80))\n",
        "a2 = torch.randn((2, 4, 8, 80, 128))"
      ],
      "metadata": {
        "id": "5I9YpSKCt2W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.matmul(a1, a2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwY6P115m8e1",
        "outputId": "803d7d79-7f27-4751-f764-f78a660e8904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 8, 14, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(torch.matmul(a1, a2) * torch.sum(w, -1, keepdims=True), dim=-2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUPqEkApxasF",
        "outputId": "92a6e333-f201-4220-ea30-23febc0b9de1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 8, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bb = torch.sum(w, -1, keepdim=True)"
      ],
      "metadata": {
        "id": "ZdYy5fm1vv9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwt3CH1Mze3Z",
        "outputId": "bb3b480e-984e-485e-eca4-9813f442f491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(({'input_ids': tensor([[     0,  16042, 174999,  56629,   2249, 146182,  26245,  32570,  50572,\n",
              "             2933,   3941,      2,      1,      1,      1,      1,      1,      1],\n",
              "          [     0,  21433, 144769,    454,    605, 191269,   3531,    524,  10587,\n",
              "            19605,   4546,    580,   4062,      2,      1,      1,      1,      1],\n",
              "          [     0,  90542,  68312,  83073,   3811,   2933,   8609,  60649,      2,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1],\n",
              "          [     0,  42812,   7630,  35459, 185165,    912, 122484,  26422,    580,\n",
              "             4062,      2,      1,      1,      1,      1,      1,      1,      1],\n",
              "          [     0, 182286, 142721, 136388,    524,   8609,  60649,   8725,  14352,\n",
              "              544,  62633,      2,      1,      1,      1,      1,      1,      1],\n",
              "          [     0,  19167,   2494,  16151,   5031,   7674,   7453,  25310,  63748,\n",
              "            23598,    449,   2059,   3042,    308,    580,   1300,      2,      1],\n",
              "          [     0,    313,  58677,    550,   3042,   3941,   2524,    524,  17998,\n",
              "            40822,      2,      1,      1,      1,      1,      1,      1,      1],\n",
              "          [     0,   9735, 143854,   3941,  18145,  10807,   3087,   3863,  24418,\n",
              "            25864,     31,   1000,  16791,   9828,  58752, 113552, 227255,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
              "  {'input_ids': tensor([[     0,  21857,   6996,      6,      4,   2249,  10305,  58403,   2494,\n",
              "            74849,  82083,   2494,  16151,    550,   8735,   5869,  32570,  50572,\n",
              "             9230,  11847,  36682,      6,      4,   2524,    524,  16193, 116067,\n",
              "              524,   6657,   5396,  36625,     53,    673,   4746,  12135, 226803,\n",
              "              550,  23630,  22446,    519,  98399,    206,   2251, 199937,     15,\n",
              "            46952,  45940,   8713, 116067, 117357,  26077,  15783,    550,  76140,\n",
              "            16042,   1907,  16042, 174999,  56629,  24885,  10657,   1388,    544,\n",
              "             4746,  21840,  42282,  62106,  56629,   2251, 186672,      6,      5,\n",
              "                2,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1],\n",
              "          [     0,  21433,    524,  18491,  14346,  22567,    427,    256,      6,\n",
              "                5,  69388,  19176,      6,      4,  19698,   3531,    524,    925,\n",
              "            10587,  19605,    580,  19698, 188592,  12613,  60479,     15,   5396,\n",
              "             3042,    524,  17998,  40822,  45886,   2933,   1388,      6,      4,\n",
              "            19698,  12613,  60479, 101390,     15,   8456,   1498,  93115,   4911,\n",
              "            60479,  91756,   1388,      6,      4,  19698,    384,  17195,    310,\n",
              "            19030,    449,    544, 189706,    310,  19030,    449,     15,   1000,\n",
              "             4194,  16389,    347,  63409,   1388,      6,      5,      2,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1],\n",
              "          [     0,  90542,  68312,  83073,     15,   3811,   2933,  47765,   1388,\n",
              "              580,    889,  18832,  46331, 101390,  25738,  56025,   5912,   3763,\n",
              "             2096,    544,   3178,   7173,   3529,   1008,   3763,   2096,      6,\n",
              "                5,  33181,   2812,    580,  13843,  21625,  13843,   8735,   6325,\n",
              "             3763,   2096,      6,      4,    345,  60479,   4603,  22394,   4570,\n",
              "             8735,   6325,    544,    893,  78206,   3763,   2096, 103100,  53655,\n",
              "                6,      4,   7899,  23907,  10895,   5869,   3763,   2096, 103100,\n",
              "            53655,     15,    780,     46,  64371,   1388,  16781,  27088,   7899,\n",
              "            23907,  10895,   5869,  17501,   8548,   2096,      6,      5,      2,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1],\n",
              "          [     0, 120933,  18832,   2546,  35459, 185165,    580,    889,  26245,\n",
              "             7899,   2546,   4797,  70879,    238,  17858,   2550,   4069,   4893,\n",
              "              550,  39005,   5869,   8215,  13503,   3763,   2096,   2059,   2781,\n",
              "            15902,  35459, 185165,      4,    912,   2781,  12552,   2249,   2933,\n",
              "            44321,  17858,   3178,   1451,   3763,   2096,  82009,    739,   1298,\n",
              "                5,  65832,  26527,      4,  17858,   3178,  10701,   3633,      4,\n",
              "            26245,   7899,   2546,   1617,   2504,   8652,   1451,      5,  50096,\n",
              "            12756,    550, 120933,  18832,   2546,  35459, 185165,   3063,   7630,\n",
              "              580,  33015,  18832,   2546,  35459, 185165,      4,    889,   4373,\n",
              "             2781,  12552,   2933,  28506,    544,   9975,   2613,   1885,   3063,\n",
              "             7630,      5,      2],\n",
              "          [     0,  88238,  21018,     19,  62105,  22049,    580,  62633,  30213,\n",
              "             6272,    550,  17501, 142721, 136388,      4,  33937,   8609,  45886,\n",
              "            22049,  15902, 142721, 136388,      4,    524,  18491,  14346,   5208,\n",
              "             6996,    580,  89725, 114999,    528,      4,  89370,    106, 104629,\n",
              "            14427,  18491,  14346,  11472,    550,  17501,    544,    580,  62633,\n",
              "              524,  18491,  14346,   7976,  11847,    138,   1000,  17501,      5,\n",
              "                2,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1],\n",
              "          [     0,  24441,  40443,   3042, 158884,  29030, 217897,    580,   1008,\n",
              "             2494,  16151,    912,  69481,  53869,   7674,   7453,  25310,  63748,\n",
              "            33537,     15,     19,  11592,  42610,    194,      2,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1],\n",
              "          [     0,    313,  58677,  18832,   5912,  10895,  56025,   5912, 133577,\n",
              "             5869,   6657,  19979,      6,  70198,     42,  70255, 130788,     15,\n",
              "                7,  46535,   2786,   1000,   4194,   6051,  27193,   1104,  10680,\n",
              "               16,    580,   5648, 174390,  16969,   2671,   2524,  33256,    889,\n",
              "            17998,  40822, 125924,      5,      2,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1],\n",
              "          [     0, 113552, 162627,  18844,   9457,   2671,    580,   5715,   1116,\n",
              "             1337,   9609,   7742,    550,   3087,      4,    581,  27366,   4597,\n",
              "              544,  27366,   4597,  15573,      5,      5,  59337,    177,    674,\n",
              "            10013,  33181,    580,   1776,  10807,   1116,   2455,   1839,  48186,\n",
              "                4,  14631, 100862,      2,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "                1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}),\n",
              " tensor([0, 1, 1, 0, 0, 1, 1, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(torch.softmax(bb, dim=-2), dim=-2)"
      ],
      "metadata": {
        "id": "UBlb2P_zzm4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(2, 2, requires_grad=True)\n",
        "y = x.view(4)  # Thay đổi kích thước tensor\n",
        "print(y.grad_fn)  # <UnsafeViewBackward object at 0x...>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxYxWO4dz1Fg",
        "outputId": "2696a3e7-bdd8-48e0-e5fd-6b1204fc2c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<ViewBackward0 object at 0x7e4777682ef0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FC_layer = nn.Linear(16, 1)"
      ],
      "metadata": {
        "id": "PUuIjLGy9rgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.randn(2, 16)"
      ],
      "metadata": {
        "id": "3o2yMi6o9v7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dhf = FC_layer(inp)"
      ],
      "metadata": {
        "id": "A8k8BHbV9zJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dhf.cpu().detach().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "JR_WIMUn93QY",
        "outputId": "96f3ab01-efd9-458f-8206-123e74d32029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-d63dd9d271d9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = torch.ones((3,4), requires_grad = True)\n",
        "\n",
        "p1 = p.permute(1,0).flatten().contiguous()\n",
        "p2 = p.transpose(1,0)\n",
        "p3 = torch.matmul(p, p2)\n",
        "p1, p2, p3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWzVoP__zUt2",
        "outputId": "fae90539-2de1-4351-fa99-4706e09aa097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        grad_fn=<UnsafeViewBackward0>),\n",
              " tensor([[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]], grad_fn=<TransposeBackward0>),\n",
              " tensor([[4., 4., 4.],\n",
              "         [4., 4., 4.],\n",
              "         [4., 4., 4.]], grad_fn=<MmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ___"
      ],
      "metadata": {
        "id": "QlXjNPsgZdfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as torch_fn"
      ],
      "metadata": {
        "id": "UToZMLAOZtZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "kYbS-OhYZe77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = nn.GELU()\n",
        "input = torch.randn(2)\n",
        "output = m(input)"
      ],
      "metadata": {
        "id": "amFxG23sZi2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output2 = torch_fn.gelu(input)"
      ],
      "metadata": {
        "id": "j75XR19YZnw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HAYGl-_Z0_m",
        "outputId": "aa0879ec-62cd-43d4-cfb7-1580aeffb431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.1217, -0.0480])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrCFF5ynZ2ju",
        "outputId": "c98d4c06-0a21-4d80-8f7f-f863dd6cb5c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.1217, -0.0480])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BM25 -> score ->\n",
        "Thêm câu hỏi\n",
        "Top > 0.95\n"
      ],
      "metadata": {
        "id": "kpx3KDnjrExs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}